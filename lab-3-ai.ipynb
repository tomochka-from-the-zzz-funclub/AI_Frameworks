{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3400552,"sourceType":"datasetVersion","datasetId":2049845},{"sourceId":7963387,"sourceType":"datasetVersion","datasetId":1328826}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Лабораторная работа №3: Проведение исследований с решающим деревом","metadata":{}},{"cell_type":"code","source":"!pip install scikit-learn numpy pandas matplotlib seaborn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:38:04.936249Z","iopub.execute_input":"2025-12-08T13:38:04.936570Z","iopub.status.idle":"2025-12-08T13:38:11.019670Z","shell.execute_reply.started":"2025-12-08T13:38:04.936544Z","shell.execute_reply":"2025-12-08T13:38:11.018334Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:38:11.022000Z","iopub.execute_input":"2025-12-08T13:38:11.022404Z","iopub.status.idle":"2025-12-08T13:38:12.575813Z","shell.execute_reply.started":"2025-12-08T13:38:11.022352Z","shell.execute_reply":"2025-12-08T13:38:12.574647Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"date_fruit_path = \"/kaggle/input/date-fruit-datasets\"\n\nprint(\"Path to Date Fruit dataset files:\", date_fruit_path)\n\n# Проверка содержимого папки\nif os.path.exists(date_fruit_path):\n    files = os.listdir(date_fruit_path)\n    print(\"Files:\", files)\nelse:\n    print(\"Not found\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:38:12.576947Z","iopub.execute_input":"2025-12-08T13:38:12.577452Z","iopub.status.idle":"2025-12-08T13:38:12.591747Z","shell.execute_reply.started":"2025-12-08T13:38:12.577425Z","shell.execute_reply":"2025-12-08T13:38:12.590568Z"}},"outputs":[{"name":"stdout","text":"Path to Date Fruit dataset files: /kaggle/input/date-fruit-datasets\nFiles: ['Date_Fruit_Datasets']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Чтение Excel-файла\ndate_fruit_data = pd.read_excel(f\"{date_fruit_path}/Date_Fruit_Datasets/Date_Fruit_Datasets.xlsx\")\n\n# Проверка данных\ndate_fruit_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:38:12.593633Z","iopub.execute_input":"2025-12-08T13:38:12.594102Z","iopub.status.idle":"2025-12-08T13:38:13.736851Z","shell.execute_reply.started":"2025-12-08T13:38:12.594075Z","shell.execute_reply":"2025-12-08T13:38:13.735919Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"     AREA  PERIMETER  MAJOR_AXIS  MINOR_AXIS  ECCENTRICITY   EQDIASQ  \\\n0  422163   2378.908    837.8484    645.6693        0.6373  733.1539   \n1  338136   2085.144    723.8198    595.2073        0.5690  656.1464   \n2  526843   2647.394    940.7379    715.3638        0.6494  819.0222   \n3  416063   2351.210    827.9804    645.2988        0.6266  727.8378   \n4  347562   2160.354    763.9877    582.8359        0.6465  665.2291   \n\n   SOLIDITY  CONVEX_AREA  EXTENT  ASPECT_RATIO  ...  KurtosisRR  KurtosisRG  \\\n0    0.9947       424428  0.7831        1.2976  ...      3.2370      2.9574   \n1    0.9974       339014  0.7795        1.2161  ...      2.6228      2.6350   \n2    0.9962       528876  0.7657        1.3150  ...      3.7516      3.8611   \n3    0.9948       418255  0.7759        1.2831  ...      5.0401      8.6136   \n4    0.9908       350797  0.7569        1.3108  ...      2.7016      2.9761   \n\n   KurtosisRB    EntropyRR    EntropyRG    EntropyRB  ALLdaub4RR  ALLdaub4RG  \\\n0      4.2287 -59191263232 -50714214400 -39922372608     58.7255     54.9554   \n1      3.1704 -34233065472 -37462601728 -31477794816     50.0259     52.8168   \n2      4.7192 -93948354560 -74738221056 -60311207936     65.4772     59.2860   \n3      8.2618 -32074307584 -32060925952 -29575010304     43.3900     44.1259   \n4      4.4146 -39980974080 -35980042240 -25593278464     52.7743     50.9080   \n\n   ALLdaub4RB  Class  \n0     47.8400  BERHI  \n1     47.8315  BERHI  \n2     51.9378  BERHI  \n3     41.1882  BERHI  \n4     42.6666  BERHI  \n\n[5 rows x 35 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AREA</th>\n      <th>PERIMETER</th>\n      <th>MAJOR_AXIS</th>\n      <th>MINOR_AXIS</th>\n      <th>ECCENTRICITY</th>\n      <th>EQDIASQ</th>\n      <th>SOLIDITY</th>\n      <th>CONVEX_AREA</th>\n      <th>EXTENT</th>\n      <th>ASPECT_RATIO</th>\n      <th>...</th>\n      <th>KurtosisRR</th>\n      <th>KurtosisRG</th>\n      <th>KurtosisRB</th>\n      <th>EntropyRR</th>\n      <th>EntropyRG</th>\n      <th>EntropyRB</th>\n      <th>ALLdaub4RR</th>\n      <th>ALLdaub4RG</th>\n      <th>ALLdaub4RB</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>422163</td>\n      <td>2378.908</td>\n      <td>837.8484</td>\n      <td>645.6693</td>\n      <td>0.6373</td>\n      <td>733.1539</td>\n      <td>0.9947</td>\n      <td>424428</td>\n      <td>0.7831</td>\n      <td>1.2976</td>\n      <td>...</td>\n      <td>3.2370</td>\n      <td>2.9574</td>\n      <td>4.2287</td>\n      <td>-59191263232</td>\n      <td>-50714214400</td>\n      <td>-39922372608</td>\n      <td>58.7255</td>\n      <td>54.9554</td>\n      <td>47.8400</td>\n      <td>BERHI</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>338136</td>\n      <td>2085.144</td>\n      <td>723.8198</td>\n      <td>595.2073</td>\n      <td>0.5690</td>\n      <td>656.1464</td>\n      <td>0.9974</td>\n      <td>339014</td>\n      <td>0.7795</td>\n      <td>1.2161</td>\n      <td>...</td>\n      <td>2.6228</td>\n      <td>2.6350</td>\n      <td>3.1704</td>\n      <td>-34233065472</td>\n      <td>-37462601728</td>\n      <td>-31477794816</td>\n      <td>50.0259</td>\n      <td>52.8168</td>\n      <td>47.8315</td>\n      <td>BERHI</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>526843</td>\n      <td>2647.394</td>\n      <td>940.7379</td>\n      <td>715.3638</td>\n      <td>0.6494</td>\n      <td>819.0222</td>\n      <td>0.9962</td>\n      <td>528876</td>\n      <td>0.7657</td>\n      <td>1.3150</td>\n      <td>...</td>\n      <td>3.7516</td>\n      <td>3.8611</td>\n      <td>4.7192</td>\n      <td>-93948354560</td>\n      <td>-74738221056</td>\n      <td>-60311207936</td>\n      <td>65.4772</td>\n      <td>59.2860</td>\n      <td>51.9378</td>\n      <td>BERHI</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>416063</td>\n      <td>2351.210</td>\n      <td>827.9804</td>\n      <td>645.2988</td>\n      <td>0.6266</td>\n      <td>727.8378</td>\n      <td>0.9948</td>\n      <td>418255</td>\n      <td>0.7759</td>\n      <td>1.2831</td>\n      <td>...</td>\n      <td>5.0401</td>\n      <td>8.6136</td>\n      <td>8.2618</td>\n      <td>-32074307584</td>\n      <td>-32060925952</td>\n      <td>-29575010304</td>\n      <td>43.3900</td>\n      <td>44.1259</td>\n      <td>41.1882</td>\n      <td>BERHI</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>347562</td>\n      <td>2160.354</td>\n      <td>763.9877</td>\n      <td>582.8359</td>\n      <td>0.6465</td>\n      <td>665.2291</td>\n      <td>0.9908</td>\n      <td>350797</td>\n      <td>0.7569</td>\n      <td>1.3108</td>\n      <td>...</td>\n      <td>2.7016</td>\n      <td>2.9761</td>\n      <td>4.4146</td>\n      <td>-39980974080</td>\n      <td>-35980042240</td>\n      <td>-25593278464</td>\n      <td>52.7743</td>\n      <td>50.9080</td>\n      <td>42.6666</td>\n      <td>BERHI</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 35 columns</p>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"concrete_strength_path = \"/kaggle/input/concrete-compressive-strength\"\n\nprint(\"Путь до датасета Concrete Compressive Strength:\", concrete_strength_path)\n\n# Проверка содержимого папки\nif os.path.exists(concrete_strength_path):\n    files = os.listdir(concrete_strength_path)\n    print(\"Содержание:\", files)\nelse:\n    print(\"Не найден\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:38:13.738029Z","iopub.execute_input":"2025-12-08T13:38:13.738541Z","iopub.status.idle":"2025-12-08T13:38:13.763054Z","shell.execute_reply.started":"2025-12-08T13:38:13.738507Z","shell.execute_reply":"2025-12-08T13:38:13.761842Z"}},"outputs":[{"name":"stdout","text":"Путь до датасета Concrete Compressive Strength: /kaggle/input/concrete-compressive-strength\nСодержание: ['Concrete Compressive Strength.csv']\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Чтение CSV-файла\nconcrete_data = pd.read_csv(f\"{concrete_strength_path}/Concrete Compressive Strength.csv\")\n\n# Проверка данных\nconcrete_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:38:13.764325Z","iopub.execute_input":"2025-12-08T13:38:13.764655Z","iopub.status.idle":"2025-12-08T13:38:13.802441Z","shell.execute_reply.started":"2025-12-08T13:38:13.764623Z","shell.execute_reply":"2025-12-08T13:38:13.801096Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n0   540.0                 0.0      0.0  162.0               2.5   \n1   540.0                 0.0      0.0  162.0               2.5   \n2   332.5               142.5      0.0  228.0               0.0   \n3   332.5               142.5      0.0  228.0               0.0   \n4   198.6               132.4      0.0  192.0               0.0   \n\n   Coarse Aggregate  Fine Aggregate  Age (day)  Concrete compressive strength   \n0            1040.0           676.0         28                       79.986111  \n1            1055.0           676.0         28                       61.887366  \n2             932.0           594.0        270                       40.269535  \n3             932.0           594.0        365                       41.052780  \n4             978.4           825.5        360                       44.296075  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age (day)</th>\n      <th>Concrete compressive strength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1040.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>79.986111</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1055.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>61.887366</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>270</td>\n      <td>40.269535</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>365</td>\n      <td>41.052780</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198.6</td>\n      <td>132.4</td>\n      <td>0.0</td>\n      <td>192.0</td>\n      <td>0.0</td>\n      <td>978.4</td>\n      <td>825.5</td>\n      <td>360</td>\n      <td>44.296075</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"### 2. Создание бейзлайна и оценка качества","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score, make_scorer\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.pipeline import Pipeline\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:38:13.803499Z","iopub.execute_input":"2025-12-08T13:38:13.803863Z","iopub.status.idle":"2025-12-08T13:38:14.357154Z","shell.execute_reply.started":"2025-12-08T13:38:13.803833Z","shell.execute_reply":"2025-12-08T13:38:14.356055Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Разделение на признаки и целевую переменную\nX_class = date_fruit_data.drop(columns=['Class'])\ny_class = date_fruit_data['Class']\n\n# Разделение на обучающую и тестовую выборки\nX_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n    X_class, y_class, test_size=0.2, random_state=42, stratify=y_class\n)\n\n# Преобразование целевой переменной\nlabel_encoder = LabelEncoder()\ny_train_class = label_encoder.fit_transform(y_train_class)\ny_test_class = label_encoder.transform(y_test_class)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:38:14.358425Z","iopub.execute_input":"2025-12-08T13:38:14.358787Z","iopub.status.idle":"2025-12-08T13:38:14.376158Z","shell.execute_reply.started":"2025-12-08T13:38:14.358760Z","shell.execute_reply":"2025-12-08T13:38:14.374754Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"Аналогично разделим датасет для регрессии","metadata":{}},{"cell_type":"code","source":"# Разделение на признаки и целевую переменную\nX_reg = concrete_data.drop(columns=['Concrete compressive strength '])\ny_reg = concrete_data['Concrete compressive strength ']\n\n# Разделение на обучающую и тестовую выборки\nX_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n    X_reg, y_reg, test_size=0.2, random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:38:14.377276Z","iopub.execute_input":"2025-12-08T13:38:14.377742Z","iopub.status.idle":"2025-12-08T13:38:14.391283Z","shell.execute_reply.started":"2025-12-08T13:38:14.377707Z","shell.execute_reply":"2025-12-08T13:38:14.389795Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"Обучим модели для классификации и регрессии из Sklearn и оценим их","metadata":{}},{"cell_type":"code","source":"tree_classifier = DecisionTreeClassifier(max_depth=5, random_state=42)\ntree_classifier.fit(X_train_class, y_train_class)\n\ny_pred_class = tree_classifier.predict(X_test_class)\n\naccuracy_tree = accuracy_score(y_test_class, y_pred_class)\nf1_tree = f1_score(y_test_class, y_pred_class, average='weighted')\n\nprint(f\"Decision Tree Classifier - Accuracy: {accuracy_tree:.4f}, F1-Score: {f1_tree:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:38:14.394684Z","iopub.execute_input":"2025-12-08T13:38:14.394986Z","iopub.status.idle":"2025-12-08T13:38:14.431393Z","shell.execute_reply.started":"2025-12-08T13:38:14.394964Z","shell.execute_reply":"2025-12-08T13:38:14.430286Z"}},"outputs":[{"name":"stdout","text":"Decision Tree Classifier - Accuracy: 0.8056, F1-Score: 0.7969\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"tree_regressor = DecisionTreeRegressor(max_depth=5, random_state=42)\ntree_regressor.fit(X_train_reg, y_train_reg)\n\ny_pred_reg = tree_regressor.predict(X_test_reg)\n\nrmse_tree = mean_squared_error(y_test_reg, y_pred_reg, squared=False)\nr2_tree = r2_score(y_test_reg, y_pred_reg)\n\nprint(f\"Decision Tree Regressor - RMSE: {rmse_tree:.4f}, R²: {r2_tree:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:38:14.432504Z","iopub.execute_input":"2025-12-08T13:38:14.432909Z","iopub.status.idle":"2025-12-08T13:38:14.451880Z","shell.execute_reply.started":"2025-12-08T13:38:14.432874Z","shell.execute_reply":"2025-12-08T13:38:14.450567Z"}},"outputs":[{"name":"stdout","text":"Decision Tree Regressor - RMSE: 9.5792, R²: 0.6439\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"Точность для встроенной в Sklearn модели решающего дерева получилась приемлемой (80.5% точности для классификатора, 9.57 RMSE для регрессора). Теперь улучшим бейзлайн и проанализируем результаты.","metadata":{}},{"cell_type":"markdown","source":"### 3. Улучшение бейзлайна\nБудем перебирать гиперпараметры max_depth, min_samples_split, min_samples_leaf решающего дерева","metadata":{}},{"cell_type":"code","source":"# 1. Подбор гиперпараметров для DecisionTreeClassifier с расширенным поиском\nprint(\"=== Decision Tree Classifier - Hyperparameter Tuning ===\")\nparam_grid_classifier = {\n    'max_depth': [2, 3, 5, 7, 10, 15, None],\n    'min_samples_split': [2, 5, 10, 15, 20],\n    'min_samples_leaf': [1, 2, 3, 4, 5],\n    'criterion': ['gini', 'entropy', 'log_loss'],\n    'max_features': ['sqrt', 'log2', None]\n}\n\ngrid_search_classifier = GridSearchCV(\n    estimator=DecisionTreeClassifier(random_state=42),\n    param_grid=param_grid_classifier,\n    scoring={'f1_weighted': make_scorer(f1_score, average='weighted'), \n             'accuracy': 'accuracy',\n             # 'precision_weighted': make_scorer(precision_score, average='weighted')\n            },\n    refit='f1_weighted',\n    cv=5,\n    verbose=1,\n    n_jobs=-1,\n    return_train_score=True\n)\n\n\ngrid_search_classifier.fit(X_train_class, y_train_class)\n\n# Лучшие параметры и результат\nbest_params_classifier = grid_search_classifier.best_params_\nbest_score_classifier = grid_search_classifier.best_score_\n\nprint(f\"\\nЛучшие параметры для классификатора: {best_params_classifier}\")\nprint(f\"Лучший F1-Score на кросс-валидации: {best_score_classifier:.4f}\")\n\n# Анализ переобучения\nbest_idx = grid_search_classifier.best_index_\ntrain_f1 = grid_search_classifier.cv_results_['mean_train_f1_weighted'][best_idx]\nval_f1 = grid_search_classifier.cv_results_['mean_test_f1_weighted'][best_idx]\nprint(f\"Train F1-Score: {train_f1:.4f}, Validation F1-Score: {val_f1:.4f}\")\nprint(f\"Разница (train - val): {train_f1 - val_f1:.4f}\")\n\n# 2. Подбор гиперпараметров для DecisionTreeRegressor с расширенным поиском\nprint(\"\\n=== Decision Tree Regressor - Hyperparameter Tuning ===\")\nparam_grid_regressor = {\n    'max_depth': [2, 3, 5, 7, 10, 15, None],\n    'min_samples_split': [2, 5, 10, 15, 20],\n    'min_samples_leaf': [1, 2, 3, 4, 5],\n    'criterion': ['squared_error', 'friedman_mse', 'absolute_error'],\n    'max_features': ['sqrt', 'log2', None, 0.5, 0.8]\n}\n\ngrid_search_regressor = GridSearchCV(\n    estimator=DecisionTreeRegressor(random_state=42),\n    param_grid=param_grid_regressor,\n    scoring={'neg_rmse': 'neg_root_mean_squared_error',\n             'neg_mae': 'neg_mean_absolute_error',\n             'r2': 'r2'},\n    refit='neg_rmse',\n    cv=5,\n    verbose=1,\n    n_jobs=-1,\n    return_train_score=True\n)\n\ngrid_search_regressor.fit(X_train_reg, y_train_reg)\n\n# Лучшие параметры и результат\nbest_params_regressor = grid_search_regressor.best_params_\nbest_score_regressor = -grid_search_regressor.best_score_\n\nprint(f\"\\nЛучшие параметры для регрессора: {best_params_regressor}\")\nprint(f\"Лучший RMSE на кросс-валидации: {best_score_regressor:.4f}\")\n\n# Дополнительная информация о лучшей модели регрессора\nbest_idx_reg = grid_search_regressor.best_index_\ntrain_rmse = -grid_search_regressor.cv_results_['mean_train_neg_rmse'][best_idx_reg]\nval_rmse = -grid_search_regressor.cv_results_['mean_test_neg_rmse'][best_idx_reg]\nr2_score_val = grid_search_regressor.cv_results_['mean_test_r2'][best_idx_reg]\n\nprint(f\"Train RMSE: {train_rmse:.4f}, Validation RMSE: {val_rmse:.4f}\")\nprint(f\"Validation R²: {r2_score_val:.4f}\")\nprint(f\"Разница RMSE (train - val): {train_rmse - val_rmse:.4f}\")\n\n# Вывод топ-3 параметров для каждой модели\nprint(\"\\n=== Топ-3 параметров для классификатора ===\")\nresults_classifier = pd.DataFrame(grid_search_classifier.cv_results_)\ntop_3_classifier = results_classifier.nsmallest(3, 'rank_test_f1_weighted')\nfor i, (_, row) in enumerate(top_3_classifier.iterrows(), 1):\n    print(f\"{i}. max_depth={row['param_max_depth']}, \"\n          f\"min_samples_split={row['param_min_samples_split']}, \"\n          f\"min_samples_leaf={row['param_min_samples_leaf']}, \"\n          f\"F1={row['mean_test_f1_weighted']:.4f}\")\n\nprint(\"\\n=== Топ-3 параметров для регрессора ===\")\nresults_regressor = pd.DataFrame(grid_search_regressor.cv_results_)\ntop_3_regressor = results_regressor.nsmallest(3, 'rank_test_neg_rmse')\nfor i, (_, row) in enumerate(top_3_regressor.iterrows(), 1):\n    print(f\"{i}. max_depth={row['param_max_depth']}, \"\n          f\"min_samples_split={row['param_min_samples_split']}, \"\n          f\"min_samples_leaf={row['param_min_samples_leaf']}, \"\n          f\"RMSE={-row['mean_test_neg_rmse']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:38:14.453076Z","iopub.execute_input":"2025-12-08T13:38:14.453330Z","iopub.status.idle":"2025-12-08T13:40:08.362948Z","shell.execute_reply.started":"2025-12-08T13:38:14.453310Z","shell.execute_reply":"2025-12-08T13:40:08.361739Z"}},"outputs":[{"name":"stdout","text":"=== Decision Tree Classifier - Hyperparameter Tuning ===\nFitting 5 folds for each of 1575 candidates, totalling 7875 fits\n\nЛучшие параметры для классификатора: {'criterion': 'entropy', 'max_depth': 10, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\nЛучший F1-Score на кросс-валидации: 0.8342\nTrain F1-Score: 0.9993, Validation F1-Score: 0.8342\nРазница (train - val): 0.1651\n\n=== Decision Tree Regressor - Hyperparameter Tuning ===\nFitting 5 folds for each of 2625 candidates, totalling 13125 fits\n\nЛучшие параметры для регрессора: {'criterion': 'friedman_mse', 'max_depth': 15, 'max_features': 0.8, 'min_samples_leaf': 1, 'min_samples_split': 2}\nЛучший RMSE на кросс-валидации: 6.8788\nTrain RMSE: 0.7426, Validation RMSE: 6.8788\nValidation R²: 0.8284\nРазница RMSE (train - val): -6.1362\n\n=== Топ-3 параметров для классификатора ===\n1. max_depth=10, min_samples_split=2, min_samples_leaf=1, F1=0.8342\n2. max_depth=10, min_samples_split=2, min_samples_leaf=1, F1=0.8342\n3. max_depth=10, min_samples_split=10, min_samples_leaf=4, F1=0.8342\n\n=== Топ-3 параметров для регрессора ===\n1. max_depth=15, min_samples_split=2, min_samples_leaf=1, RMSE=6.8788\n2. max_depth=10, min_samples_split=2, min_samples_leaf=3, RMSE=6.8904\n3. max_depth=10, min_samples_split=5, min_samples_leaf=3, RMSE=6.8904\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"Обучим модели с лучшими гиперпараметрами","metadata":{}},{"cell_type":"code","source":"# 1. Обучение DecisionTreeClassifier с лучшими параметрами\nprint(\"\\n=== Обучение Decision Tree Classifier с лучшими параметрами ===\")\nbest_tree_classifier = DecisionTreeClassifier(\n    max_depth=best_params_classifier['max_depth'],\n    min_samples_split=best_params_classifier['min_samples_split'],\n    min_samples_leaf=best_params_classifier['min_samples_leaf'],\n    random_state=42\n)\n\nbest_tree_classifier.fit(X_train_class, y_train_class)\n\n# Предсказания на тестовой выборке\ny_pred_class = best_tree_classifier.predict(X_test_class)\n\n# Оценка качества\naccuracy = accuracy_score(y_test_class, y_pred_class)\nf1 = f1_score(y_test_class, y_pred_class, average='weighted')\n\nprint(f\"Test Accuracy: {accuracy:.4f}\")\nprint(f\"Test F1-Score: {f1:.4f}\")\n\n# 2. Обучение DecisionTreeRegressor с лучшими параметрами\nprint(\"\\n=== Обучение Decision Tree Regressor с лучшими параметрами ===\")\nbest_tree_regressor = DecisionTreeRegressor(\n    max_depth=best_params_regressor['max_depth'],\n    min_samples_split=best_params_regressor['min_samples_split'],\n    min_samples_leaf=best_params_regressor['min_samples_leaf'],\n    random_state=42\n)\n\nbest_tree_regressor.fit(X_train_reg, y_train_reg)\n\n# Предсказания на тестовой выборке\ny_pred_reg = best_tree_regressor.predict(X_test_reg)\n\n# Оценка качества\nrmse = mean_squared_error(y_test_reg, y_pred_reg, squared=False)\nr2 = r2_score(y_test_reg, y_pred_reg)\n\nprint(f\"Test RMSE: {rmse:.4f}\")\nprint(f\"Test R²: {r2:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:40:08.364107Z","iopub.execute_input":"2025-12-08T13:40:08.364443Z","iopub.status.idle":"2025-12-08T13:40:08.405650Z","shell.execute_reply.started":"2025-12-08T13:40:08.364414Z","shell.execute_reply":"2025-12-08T13:40:08.404407Z"}},"outputs":[{"name":"stdout","text":"\n=== Обучение Decision Tree Classifier с лучшими параметрами ===\nTest Accuracy: 0.8556\nTest F1-Score: 0.8587\n\n=== Обучение Decision Tree Regressor с лучшими параметрами ===\nTest RMSE: 6.8943\nTest R²: 0.8155\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"Точность обоих моделей возросла, причем для регрессора - существенно (RMSE 6.89 против 9.57)","metadata":{}},{"cell_type":"markdown","source":"### 4. Имплементация алгоритма машинного обучения\nНапишем собственную реализацию решающего дерева для классификации и регрессии, затем обучим модели на тестовых данных и сравним по качеству с реализациями из Sklearn.","metadata":{}},{"cell_type":"code","source":"class DecisionTreeNode:\n    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None, samples=None):\n        self.feature = feature          # Признак для разбиения\n        self.threshold = threshold      # Пороговое значение\n        self.left = left                # Левое поддерево\n        self.right = right              # Правое поддерево\n        self.value = value              # Значение в листе\n        self.samples = samples          # Количество образцов в узле\n\nclass DecisionTreeClassifierCustom:\n    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1, \n                 criterion='gini', max_features=None, random_state=None):\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.criterion = criterion\n        self.max_features = max_features\n        self.random_state = random_state\n        self.root = None\n        self.n_features_ = None\n        self.n_classes_ = None\n        \n        if random_state is not None:\n            np.random.seed(random_state)\n\n    def _calculate_impurity(self, y):\n        \"\"\"Вычисление критерия неоднородности\"\"\"\n        if self.criterion == 'gini':\n            proportions = np.bincount(y) / len(y)\n            return 1 - np.sum(proportions ** 2)\n        elif self.criterion == 'entropy':\n            proportions = np.bincount(y) / len(y)\n            proportions = proportions[proportions > 0]  # Избегаем log(0)\n            return -np.sum(proportions * np.log2(proportions))\n        else:\n            raise ValueError(f\"Неизвестный критерий: {self.criterion}\")\n\n    def _split(self, X, y, feature, threshold):\n        \"\"\"Разделение данных по признаку и порогу\"\"\"\n        left_mask = X[:, feature] < threshold\n        right_mask = ~left_mask\n        return X[left_mask], y[left_mask], X[right_mask], y[right_mask]\n\n    def _best_split(self, X, y):\n        \"\"\"Поиск лучшего разбиения с учетом max_features\"\"\"\n        best_gain = -float('inf')\n        best_split = None\n        \n        # Выбор подмножества признаков для поиска\n        n_features = X.shape[1]\n        if self.max_features == 'sqrt':\n            max_features = int(np.sqrt(n_features))\n        elif self.max_features == 'log2':\n            max_features = int(np.log2(n_features))\n        elif isinstance(self.max_features, float):\n            max_features = int(self.max_features * n_features)\n        elif self.max_features is None:\n            max_features = n_features\n        else:\n            max_features = self.max_features\n            \n        # Выбор случайного подмножества признаков\n        feature_indices = np.random.choice(n_features, min(max_features, n_features), replace=False)\n        \n        current_impurity = self._calculate_impurity(y)\n        \n        for feature in feature_indices:\n            thresholds = np.unique(X[:, feature])\n            \n            # Случайная выборка порогов для ускорения\n            if len(thresholds) > 20:\n                thresholds = np.random.choice(thresholds, 20, replace=False)\n                \n            for threshold in thresholds:\n                X_left, y_left, X_right, y_right = self._split(X, y, feature, threshold)\n                \n                # Проверка минимального количества образцов\n                if len(y_left) < self.min_samples_leaf or len(y_right) < self.min_samples_leaf:\n                    continue\n                \n                # Вычисление gain\n                impurity_left = self._calculate_impurity(y_left)\n                impurity_right = self._calculate_impurity(y_right)\n                \n                n_left, n_right = len(y_left), len(y_right)\n                n_total = n_left + n_right\n                \n                gain = current_impurity - (n_left/n_total * impurity_left + n_right/n_total * impurity_right)\n                \n                if gain > best_gain:\n                    best_gain = gain\n                    best_split = (feature, threshold, gain)\n\n        return best_split\n\n    def _build_tree(self, X, y, depth):\n        \"\"\"Рекурсивное построение дерева\"\"\"\n        n_samples = len(y)\n        \n        # Условия остановки\n        if (depth == self.max_depth or \n            n_samples < self.min_samples_split or \n            len(np.unique(y)) == 1):\n            \n            # Находим наиболее частый класс\n            unique, counts = np.unique(y, return_counts=True)\n            value = unique[np.argmax(counts)]\n            return DecisionTreeNode(value=value, samples=n_samples)\n\n        # Поиск лучшего разбиения\n        split_result = self._best_split(X, y)\n        if split_result is None:\n            unique, counts = np.unique(y, return_counts=True)\n            value = unique[np.argmax(counts)]\n            return DecisionTreeNode(value=value, samples=n_samples)\n        \n        feature, threshold, gain = split_result\n        \n        # Разделение данных\n        X_left, y_left, X_right, y_right = self._split(X, y, feature, threshold)\n        \n        # Проверка минимального количества образцов\n        if len(y_left) < self.min_samples_leaf or len(y_right) < self.min_samples_leaf:\n            unique, counts = np.unique(y, return_counts=True)\n            value = unique[np.argmax(counts)]\n            return DecisionTreeNode(value=value, samples=n_samples)\n        \n        # Рекурсивное построение поддеревьев\n        left_child = self._build_tree(X_left, y_left, depth + 1)\n        right_child = self._build_tree(X_right, y_right, depth + 1)\n        \n        return DecisionTreeNode(feature=feature, threshold=threshold, \n                              left=left_child, right=right_child, samples=n_samples)\n\n    def fit(self, X, y):\n        \"\"\"Обучение модели\"\"\"\n        self.n_features_ = X.shape[1]\n        self.n_classes_ = len(np.unique(y))\n        self.root = self._build_tree(np.array(X), np.array(y), 0)\n        return self\n\n    def _predict_one(self, x, node):\n        \"\"\"Предсказание для одного образца\"\"\"\n        if node.value is not None:\n            return node.value\n        if x[node.feature] < node.threshold:\n            return self._predict_one(x, node.left)\n        else:\n            return self._predict_one(x, node.right)\n\n    def predict(self, X):\n        \"\"\"Предсказание для набора данных\"\"\"\n        return np.array([self._predict_one(x, self.root) for x in X])\n\n    def predict_proba(self, X):\n        \"\"\"Вероятностные предсказания\"\"\"\n        predictions = []\n        for x in X:\n            node = self.root\n            while node.value is None:\n                if x[node.feature] < node.threshold:\n                    node = node.left\n                else:\n                    node = node.right\n            \n            # Создаем вектор вероятностей\n            proba = np.zeros(self.n_classes_)\n            proba[int(node.value)] = 1.0\n            predictions.append(proba)\n        \n        return np.array(predictions)\n\n\nclass DecisionTreeRegressorCustom:\n    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1,\n                 criterion='mse', max_features=None, random_state=None):\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.criterion = criterion\n        self.max_features = max_features\n        self.random_state = random_state\n        self.root = None\n        self.n_features_ = None\n        \n        if random_state is not None:\n            np.random.seed(random_state)\n\n    def _calculate_loss(self, y):\n        \"\"\"Вычисление функции потерь\"\"\"\n        if self.criterion == 'mse':\n            return np.mean((y - np.mean(y)) ** 2)\n        elif self.criterion == 'mae':\n            return np.mean(np.abs(y - np.median(y)))\n        else:\n            raise ValueError(f\"Неизвестный критерий: {self.criterion}\")\n\n    def _split(self, X, y, feature, threshold):\n        \"\"\"Разделение данных по признаку и порогу\"\"\"\n        left_mask = X[:, feature] < threshold\n        right_mask = ~left_mask\n        return X[left_mask], y[left_mask], X[right_mask], y[right_mask]\n\n    def _best_split(self, X, y):\n        \"\"\"Поиск лучшего разбиения\"\"\"\n        best_gain = -float('inf')\n        best_split = None\n        \n        current_loss = self._calculate_loss(y)\n        \n        # Выбор подмножества признаков\n        n_features = X.shape[1]\n        if self.max_features == 'sqrt':\n            max_features = int(np.sqrt(n_features))\n        elif self.max_features == 'log2':\n            max_features = int(np.log2(n_features))\n        elif isinstance(self.max_features, float):\n            max_features = int(self.max_features * n_features)\n        elif self.max_features is None:\n            max_features = n_features\n        else:\n            max_features = self.max_features\n            \n        feature_indices = np.random.choice(n_features, min(max_features, n_features), replace=False)\n        \n        for feature in feature_indices:\n            thresholds = np.unique(X[:, feature])\n            \n            # Случайная выборка порогов для ускорения\n            if len(thresholds) > 20:\n                thresholds = np.random.choice(thresholds, 20, replace=False)\n                \n            for threshold in thresholds:\n                X_left, y_left, X_right, y_right = self._split(X, y, feature, threshold)\n                \n                if len(y_left) < self.min_samples_leaf or len(y_right) < self.min_samples_leaf:\n                    continue\n                \n                loss_left = self._calculate_loss(y_left)\n                loss_right = self._calculate_loss(y_right)\n                \n                n_left, n_right = len(y_left), len(y_right)\n                n_total = n_left + n_right\n                \n                gain = current_loss - (n_left/n_total * loss_left + n_right/n_total * loss_right)\n                \n                if gain > best_gain:\n                    best_gain = gain\n                    best_split = (feature, threshold, gain)\n\n        return best_split\n\n    def _build_tree(self, X, y, depth):\n        \"\"\"Рекурсивное построение дерева\"\"\"\n        n_samples = len(y)\n        \n        # Условия остановки\n        if (depth == self.max_depth or \n            n_samples < self.min_samples_split or \n            self._calculate_loss(y) < 1e-10):\n            \n            if self.criterion == 'mse':\n                value = np.mean(y)\n            else:\n                value = np.median(y)\n            return DecisionTreeNode(value=value, samples=n_samples)\n\n        # Поиск лучшего разбиения\n        split_result = self._best_split(X, y)\n        if split_result is None:\n            if self.criterion == 'mse':\n                value = np.mean(y)\n            else:\n                value = np.median(y)\n            return DecisionTreeNode(value=value, samples=n_samples)\n        \n        feature, threshold, gain = split_result\n        \n        # Разделение данных\n        X_left, y_left, X_right, y_right = self._split(X, y, feature, threshold)\n        \n        if len(y_left) < self.min_samples_leaf or len(y_right) < self.min_samples_leaf:\n            if self.criterion == 'mse':\n                value = np.mean(y)\n            else:\n                value = np.median(y)\n            return DecisionTreeNode(value=value, samples=n_samples)\n        \n        # Рекурсивное построение поддеревьев\n        left_child = self._build_tree(X_left, y_left, depth + 1)\n        right_child = self._build_tree(X_right, y_right, depth + 1)\n        \n        return DecisionTreeNode(feature=feature, threshold=threshold, \n                              left=left_child, right=right_child, samples=n_samples)\n\n    def fit(self, X, y):\n        \"\"\"Обучение модели\"\"\"\n        self.n_features_ = X.shape[1]\n        self.root = self._build_tree(np.array(X), np.array(y), 0)\n        return self\n\n    def _predict_one(self, x, node):\n        \"\"\"Предсказание для одного образца\"\"\"\n        if node.value is not None:\n            return node.value\n        if x[node.feature] < node.threshold:\n            return self._predict_one(x, node.left)\n        else:\n            return self._predict_one(x, node.right)\n\n    def predict(self, X):\n        \"\"\"Предсказание для набора данных\"\"\"\n        return np.array([self._predict_one(x, self.root) for x in X])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:40:08.406791Z","iopub.execute_input":"2025-12-08T13:40:08.407024Z","iopub.status.idle":"2025-12-08T13:40:08.445255Z","shell.execute_reply.started":"2025-12-08T13:40:08.407007Z","shell.execute_reply":"2025-12-08T13:40:08.444000Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"Обучим модели и оценим их качество","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nX_train_class = scaler.fit_transform(X_train_class)\nX_test_class = scaler.transform(X_test_class)\n\ncustom_tree_classifier = DecisionTreeClassifierCustom(max_depth=5, min_samples_split=2)\ncustom_tree_classifier.fit(X_train_class, y_train_class)\n\ny_pred_class = custom_tree_classifier.predict(X_test_class)\naccuracy = accuracy_score(y_test_class, y_pred_class)\nf1 = f1_score(y_test_class, y_pred_class, average='weighted')\n\nprint(f\"Custom Decision Tree Classifier - Accuracy: {accuracy:.4f}, F1-Score: {f1:.4f}\")\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T13:40:08.446220Z","iopub.execute_input":"2025-12-08T13:40:08.446501Z","iopub.status.idle":"2025-12-08T13:40:08.907997Z","shell.execute_reply.started":"2025-12-08T13:40:08.446479Z","shell.execute_reply":"2025-12-08T13:40:08.906743Z"}},"outputs":[{"name":"stdout","text":"Custom Decision Tree Classifier - Accuracy: 0.8389, F1-Score: 0.8429\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"scaler = StandardScaler()\nX_train_reg = scaler.fit_transform(X_train_reg)\nX_test_reg = scaler.transform(X_test_reg)\n\ncustom_tree_regressor = DecisionTreeRegressorCustom(max_depth=5, min_samples_split=2)\ncustom_tree_regressor.fit(X_train_reg, np.array(y_train_reg))\n\ny_pred_reg = custom_tree_regressor.predict(X_test_reg)\nrmse = mean_squared_error(y_test_reg, y_pred_reg, squared=False)\nr2 = r2_score(y_test_reg, y_pred_reg)\n\nprint(f\"Custom Decision Tree Regressor - RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n     \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T14:01:39.000608Z","iopub.execute_input":"2025-12-08T14:01:39.001233Z","iopub.status.idle":"2025-12-08T14:01:39.328753Z","shell.execute_reply.started":"2025-12-08T14:01:39.001200Z","shell.execute_reply":"2025-12-08T14:01:39.325186Z"}},"outputs":[{"name":"stdout","text":"Custom Decision Tree Regressor - RMSE: 8.4854, R²: 0.7206\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"Точность реализованных вручную моделей близка к библиотечным.","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
