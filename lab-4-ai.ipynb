{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3400552,"sourceType":"datasetVersion","datasetId":2049845},{"sourceId":7963387,"sourceType":"datasetVersion","datasetId":1328826}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Лабораторная работа №4: Проведение исследований со случайным лесом\n","metadata":{}},{"cell_type":"markdown","source":"### 1. Выбор начальных условий","metadata":{}},{"cell_type":"markdown","source":"Был проведен в ЛР №1.","metadata":{}},{"cell_type":"code","source":"!pip install scikit-learn numpy pandas matplotlib seaborn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T20:24:59.691469Z","iopub.execute_input":"2025-12-14T20:24:59.691764Z","iopub.status.idle":"2025-12-14T20:25:05.743559Z","shell.execute_reply.started":"2025-12-14T20:24:59.691743Z","shell.execute_reply":"2025-12-14T20:25:05.742202Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T20:25:05.745536Z","iopub.execute_input":"2025-12-14T20:25:05.745862Z","iopub.status.idle":"2025-12-14T20:25:06.647973Z","shell.execute_reply.started":"2025-12-14T20:25:05.745808Z","shell.execute_reply":"2025-12-14T20:25:06.646858Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"date_fruit_path = \"/kaggle/input/date-fruit-datasets\"\n\nprint(\"Path to Date Fruit dataset files:\", date_fruit_path)\n\n# Проверка содержимого папки\nif os.path.exists(date_fruit_path):\n    files = os.listdir(date_fruit_path)\n    print(\"Files:\", files)\nelse:\n    print(\"Not found\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T20:25:06.648934Z","iopub.execute_input":"2025-12-14T20:25:06.649351Z","iopub.status.idle":"2025-12-14T20:25:06.660437Z","shell.execute_reply.started":"2025-12-14T20:25:06.649326Z","shell.execute_reply":"2025-12-14T20:25:06.659129Z"}},"outputs":[{"name":"stdout","text":"Path to Date Fruit dataset files: /kaggle/input/date-fruit-datasets\nFiles: ['Date_Fruit_Datasets']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Чтение Excel-файла\ndate_fruit_data = pd.read_excel(f\"{date_fruit_path}/Date_Fruit_Datasets/Date_Fruit_Datasets.xlsx\")\n\n# Проверка данных\ndate_fruit_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T20:25:06.662593Z","iopub.execute_input":"2025-12-14T20:25:06.663008Z","iopub.status.idle":"2025-12-14T20:25:07.899704Z","shell.execute_reply.started":"2025-12-14T20:25:06.662984Z","shell.execute_reply":"2025-12-14T20:25:07.898457Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"     AREA  PERIMETER  MAJOR_AXIS  MINOR_AXIS  ECCENTRICITY   EQDIASQ  \\\n0  422163   2378.908    837.8484    645.6693        0.6373  733.1539   \n1  338136   2085.144    723.8198    595.2073        0.5690  656.1464   \n2  526843   2647.394    940.7379    715.3638        0.6494  819.0222   \n3  416063   2351.210    827.9804    645.2988        0.6266  727.8378   \n4  347562   2160.354    763.9877    582.8359        0.6465  665.2291   \n\n   SOLIDITY  CONVEX_AREA  EXTENT  ASPECT_RATIO  ...  KurtosisRR  KurtosisRG  \\\n0    0.9947       424428  0.7831        1.2976  ...      3.2370      2.9574   \n1    0.9974       339014  0.7795        1.2161  ...      2.6228      2.6350   \n2    0.9962       528876  0.7657        1.3150  ...      3.7516      3.8611   \n3    0.9948       418255  0.7759        1.2831  ...      5.0401      8.6136   \n4    0.9908       350797  0.7569        1.3108  ...      2.7016      2.9761   \n\n   KurtosisRB    EntropyRR    EntropyRG    EntropyRB  ALLdaub4RR  ALLdaub4RG  \\\n0      4.2287 -59191263232 -50714214400 -39922372608     58.7255     54.9554   \n1      3.1704 -34233065472 -37462601728 -31477794816     50.0259     52.8168   \n2      4.7192 -93948354560 -74738221056 -60311207936     65.4772     59.2860   \n3      8.2618 -32074307584 -32060925952 -29575010304     43.3900     44.1259   \n4      4.4146 -39980974080 -35980042240 -25593278464     52.7743     50.9080   \n\n   ALLdaub4RB  Class  \n0     47.8400  BERHI  \n1     47.8315  BERHI  \n2     51.9378  BERHI  \n3     41.1882  BERHI  \n4     42.6666  BERHI  \n\n[5 rows x 35 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AREA</th>\n      <th>PERIMETER</th>\n      <th>MAJOR_AXIS</th>\n      <th>MINOR_AXIS</th>\n      <th>ECCENTRICITY</th>\n      <th>EQDIASQ</th>\n      <th>SOLIDITY</th>\n      <th>CONVEX_AREA</th>\n      <th>EXTENT</th>\n      <th>ASPECT_RATIO</th>\n      <th>...</th>\n      <th>KurtosisRR</th>\n      <th>KurtosisRG</th>\n      <th>KurtosisRB</th>\n      <th>EntropyRR</th>\n      <th>EntropyRG</th>\n      <th>EntropyRB</th>\n      <th>ALLdaub4RR</th>\n      <th>ALLdaub4RG</th>\n      <th>ALLdaub4RB</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>422163</td>\n      <td>2378.908</td>\n      <td>837.8484</td>\n      <td>645.6693</td>\n      <td>0.6373</td>\n      <td>733.1539</td>\n      <td>0.9947</td>\n      <td>424428</td>\n      <td>0.7831</td>\n      <td>1.2976</td>\n      <td>...</td>\n      <td>3.2370</td>\n      <td>2.9574</td>\n      <td>4.2287</td>\n      <td>-59191263232</td>\n      <td>-50714214400</td>\n      <td>-39922372608</td>\n      <td>58.7255</td>\n      <td>54.9554</td>\n      <td>47.8400</td>\n      <td>BERHI</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>338136</td>\n      <td>2085.144</td>\n      <td>723.8198</td>\n      <td>595.2073</td>\n      <td>0.5690</td>\n      <td>656.1464</td>\n      <td>0.9974</td>\n      <td>339014</td>\n      <td>0.7795</td>\n      <td>1.2161</td>\n      <td>...</td>\n      <td>2.6228</td>\n      <td>2.6350</td>\n      <td>3.1704</td>\n      <td>-34233065472</td>\n      <td>-37462601728</td>\n      <td>-31477794816</td>\n      <td>50.0259</td>\n      <td>52.8168</td>\n      <td>47.8315</td>\n      <td>BERHI</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>526843</td>\n      <td>2647.394</td>\n      <td>940.7379</td>\n      <td>715.3638</td>\n      <td>0.6494</td>\n      <td>819.0222</td>\n      <td>0.9962</td>\n      <td>528876</td>\n      <td>0.7657</td>\n      <td>1.3150</td>\n      <td>...</td>\n      <td>3.7516</td>\n      <td>3.8611</td>\n      <td>4.7192</td>\n      <td>-93948354560</td>\n      <td>-74738221056</td>\n      <td>-60311207936</td>\n      <td>65.4772</td>\n      <td>59.2860</td>\n      <td>51.9378</td>\n      <td>BERHI</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>416063</td>\n      <td>2351.210</td>\n      <td>827.9804</td>\n      <td>645.2988</td>\n      <td>0.6266</td>\n      <td>727.8378</td>\n      <td>0.9948</td>\n      <td>418255</td>\n      <td>0.7759</td>\n      <td>1.2831</td>\n      <td>...</td>\n      <td>5.0401</td>\n      <td>8.6136</td>\n      <td>8.2618</td>\n      <td>-32074307584</td>\n      <td>-32060925952</td>\n      <td>-29575010304</td>\n      <td>43.3900</td>\n      <td>44.1259</td>\n      <td>41.1882</td>\n      <td>BERHI</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>347562</td>\n      <td>2160.354</td>\n      <td>763.9877</td>\n      <td>582.8359</td>\n      <td>0.6465</td>\n      <td>665.2291</td>\n      <td>0.9908</td>\n      <td>350797</td>\n      <td>0.7569</td>\n      <td>1.3108</td>\n      <td>...</td>\n      <td>2.7016</td>\n      <td>2.9761</td>\n      <td>4.4146</td>\n      <td>-39980974080</td>\n      <td>-35980042240</td>\n      <td>-25593278464</td>\n      <td>52.7743</td>\n      <td>50.9080</td>\n      <td>42.6666</td>\n      <td>BERHI</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 35 columns</p>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"concrete_strength_path = \"/kaggle/input/concrete-compressive-strength\"\nprint(\"Путь до датасета Concrete Compressive Strength:\", concrete_strength_path)\nif os.path.exists(concrete_strength_path):\n    files = os.listdir(concrete_strength_path)\n    print(\"Содержание:\", files)\nelse:\n    print(\"Не найден\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T20:25:07.901439Z","iopub.execute_input":"2025-12-14T20:25:07.901941Z","iopub.status.idle":"2025-12-14T20:25:07.911985Z","shell.execute_reply.started":"2025-12-14T20:25:07.901912Z","shell.execute_reply":"2025-12-14T20:25:07.911146Z"}},"outputs":[{"name":"stdout","text":"Путь до датасета Concrete Compressive Strength: /kaggle/input/concrete-compressive-strength\nСодержание: ['Concrete Compressive Strength.csv']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Чтение CSV-файла\nconcrete_data = pd.read_csv(f\"{concrete_strength_path}/Concrete Compressive Strength.csv\")\n\n# Проверка данных\nconcrete_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T20:25:07.912971Z","iopub.execute_input":"2025-12-14T20:25:07.913499Z","iopub.status.idle":"2025-12-14T20:25:07.951736Z","shell.execute_reply.started":"2025-12-14T20:25:07.913475Z","shell.execute_reply":"2025-12-14T20:25:07.950640Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n0   540.0                 0.0      0.0  162.0               2.5   \n1   540.0                 0.0      0.0  162.0               2.5   \n2   332.5               142.5      0.0  228.0               0.0   \n3   332.5               142.5      0.0  228.0               0.0   \n4   198.6               132.4      0.0  192.0               0.0   \n\n   Coarse Aggregate  Fine Aggregate  Age (day)  Concrete compressive strength   \n0            1040.0           676.0         28                       79.986111  \n1            1055.0           676.0         28                       61.887366  \n2             932.0           594.0        270                       40.269535  \n3             932.0           594.0        365                       41.052780  \n4             978.4           825.5        360                       44.296075  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age (day)</th>\n      <th>Concrete compressive strength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1040.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>79.986111</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1055.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>61.887366</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>270</td>\n      <td>40.269535</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>365</td>\n      <td>41.052780</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198.6</td>\n      <td>132.4</td>\n      <td>0.0</td>\n      <td>192.0</td>\n      <td>0.0</td>\n      <td>978.4</td>\n      <td>825.5</td>\n      <td>360</td>\n      <td>44.296075</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"### 2. Создание бейзлайна и оценка качества","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score, make_scorer\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.pipeline import Pipeline\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T20:25:07.952746Z","iopub.execute_input":"2025-12-14T20:25:07.953057Z","iopub.status.idle":"2025-12-14T20:25:08.453881Z","shell.execute_reply.started":"2025-12-14T20:25:07.953034Z","shell.execute_reply":"2025-12-14T20:25:08.452549Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"Разделим датасет для классификации на обучающую и тестовую выборки\n","metadata":{}},{"cell_type":"code","source":"# Разделение на признаки и целевую переменную\nX_class = date_fruit_data.drop(columns=['Class'])\ny_class = date_fruit_data['Class']\n\n# Разделение на обучающую и тестовую выборки\nX_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n    X_class, y_class, test_size=0.2, random_state=42, stratify=y_class\n)\n\n# Преобразование целевой переменной\nlabel_encoder = LabelEncoder()\ny_train_class = label_encoder.fit_transform(y_train_class)\ny_test_class = label_encoder.transform(y_test_class)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T20:25:08.455181Z","iopub.execute_input":"2025-12-14T20:25:08.455593Z","iopub.status.idle":"2025-12-14T20:25:08.479037Z","shell.execute_reply.started":"2025-12-14T20:25:08.455560Z","shell.execute_reply":"2025-12-14T20:25:08.476938Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"Аналогично разделим датасет для регрессии","metadata":{}},{"cell_type":"code","source":"# Разделение на признаки и целевую переменную\nX_reg = concrete_data.drop(columns=['Concrete compressive strength '])\ny_reg = concrete_data['Concrete compressive strength ']\n\n# Разделение на обучающую и тестовую выборки\nX_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n    X_reg, y_reg, test_size=0.2, random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T20:25:08.480633Z","iopub.execute_input":"2025-12-14T20:25:08.481008Z","iopub.status.idle":"2025-12-14T20:25:08.508208Z","shell.execute_reply.started":"2025-12-14T20:25:08.480976Z","shell.execute_reply":"2025-12-14T20:25:08.506647Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"Обучим модели для классификации и регрессии из Sklearn и оценим их","metadata":{}},{"cell_type":"code","source":"rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\nrf_classifier.fit(X_train_class, y_train_class)\n\ny_pred_class = rf_classifier.predict(X_test_class)\n\naccuracy_rf = accuracy_score(y_test_class, y_pred_class)\nf1_rf = f1_score(y_test_class, y_pred_class, average='weighted')\n\nprint(f\"Accuracy: {accuracy_rf:.4f}, F1-Score: {f1_rf:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T20:25:08.512234Z","iopub.execute_input":"2025-12-14T20:25:08.512591Z","iopub.status.idle":"2025-12-14T20:25:08.883283Z","shell.execute_reply.started":"2025-12-14T20:25:08.512545Z","shell.execute_reply":"2025-12-14T20:25:08.882153Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.9222, F1-Score: 0.9214\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"rf_regressor = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\nrf_regressor.fit(X_train_reg, y_train_reg)\n\ny_pred_reg = rf_regressor.predict(X_test_reg)\n\nrmse_rf = mean_squared_error(y_test_reg, y_pred_reg, squared=False)\nr2_rf = r2_score(y_test_reg, y_pred_reg)\n\nprint(f\"RMSE: {rmse_rf:.4f}, R²: {r2_rf:.4f}\")\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T20:25:08.884566Z","iopub.execute_input":"2025-12-14T20:25:08.884955Z","iopub.status.idle":"2025-12-14T20:25:09.289339Z","shell.execute_reply.started":"2025-12-14T20:25:08.884926Z","shell.execute_reply":"2025-12-14T20:25:09.287996Z"}},"outputs":[{"name":"stdout","text":"RMSE: 5.5318, R²: 0.8812\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"Точность для встроенной в Sklearn модели случайного леса получилась высокой, в частности для регрессора. Для алгоритма KNN результаты были Accuracy: 0.7111, RMSE: 8.2983, что сильно хуже.Попробуем улучшить бейзлайн.","metadata":{}},{"cell_type":"markdown","source":"### 3. Улучшение бейзлайна","metadata":{}},{"cell_type":"markdown","source":"Будем перебирать гиперпараметры случайного леса, такие, как n_estimators, max_depth, min_samples_split, min_samples_leaf","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score\n\n# Улучшенные параметры сетки с логической прогрессией\nparam_grid_classifier = {\n    'n_estimators': [50, 100, 150],\n    'max_depth': [5, 10, 15, None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['sqrt', 'log2']\n}\n\nparam_grid_regressor = {\n    'n_estimators': [50, 100, 150],\n    'max_depth': [5, 10, 15, None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': [0.5, 0.8, 'sqrt']\n}\n\n# Подбор гиперпараметров для RandomForestClassifier \nprint(\"=== Random Forest Classifier - Hyperparameter Tuning ===\")\ngrid_search_rf_classifier = GridSearchCV(\n    estimator=RandomForestClassifier(random_state=42, n_jobs=-1),\n    param_grid=param_grid_classifier,\n    scoring='accuracy',\n    cv=5,\n    verbose=1,\n    n_jobs=-1,\n    refit=True\n)\n\ngrid_search_rf_classifier.fit(X_train_class, y_train_class)\n\n# Лучшие параметры и результат на кросс-валидации\nbest_params_rf_classifier = grid_search_rf_classifier.best_params_\nbest_cv_score_rf_classifier = grid_search_rf_classifier.best_score_\n\nprint(f\"Лучшие параметры для Random Forest Classifier: {best_params_rf_classifier}\")\nprint(f\"Лучший Accuracy (кросс-валидация): {best_cv_score_rf_classifier:.4f}\")\n\n# Оценка на тестовой выборке\nbest_rf_classifier = grid_search_rf_classifier.best_estimator_\ny_pred_class_test = best_rf_classifier.predict(X_test_class)\n\naccuracy_test = accuracy_score(y_test_class, y_pred_class_test)\nf1_test = f1_score(y_test_class, y_pred_class_test, average='weighted')\n\nprint(f\"\\n=== Результаты на тестовой выборке ===\")\nprint(f\"Test Accuracy: {accuracy_test:.4f}\")\nprint(f\"Test F1-Score: {f1_test:.4f}\")\n\n# Подбор гиперпараметров для RandomForestRegressor \nprint(\"\\n=== Random Forest Regressor - Hyperparameter Tuning ===\")\ngrid_search_rf_regressor = GridSearchCV(\n    estimator=RandomForestRegressor(random_state=42),\n    param_grid=param_grid_regressor,\n    scoring='neg_root_mean_squared_error',\n    cv=3,\n    verbose=1,\n    n_jobs=-1,\n    refit=True\n)\n\ngrid_search_rf_regressor.fit(X_train_reg, y_train_reg)\n\n# Лучшие параметры и результат на кросс-валидации\nbest_params_rf_regressor = grid_search_rf_regressor.best_params_\nbest_cv_score_rf_regressor = -grid_search_rf_regressor.best_score_\n\nprint(f\"Лучшие параметры для Random Forest Regressor: {best_params_rf_regressor}\")\nprint(f\"Лучший RMSE (кросс-валидация): {best_cv_score_rf_regressor:.4f}\")\n\n# Оценка на тестовой выборке (используем лучшую модель из GridSearchCV)\nbest_rf_regressor = grid_search_rf_regressor.best_estimator_\ny_pred_reg_test = best_rf_regressor.predict(X_test_reg)\n\nrmse_test = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg_test))\nr2_test = r2_score(y_test_reg, y_pred_reg_test)\nmae_test = np.mean(np.abs(y_test_reg - y_pred_reg_test))\n\nprint(f\"\\n=== Результаты на тестовой выборке ===\")\nprint(f\"Test RMSE: {rmse_test:.4f}\")\nprint(f\"Test R²: {r2_test:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T20:25:09.290288Z","iopub.execute_input":"2025-12-14T20:25:09.290563Z","iopub.status.idle":"2025-12-14T20:28:55.570102Z","shell.execute_reply.started":"2025-12-14T20:25:09.290541Z","shell.execute_reply":"2025-12-14T20:28:55.568689Z"}},"outputs":[{"name":"stdout","text":"=== Random Forest Classifier - Hyperparameter Tuning ===\nFitting 5 folds for each of 216 candidates, totalling 1080 fits\nЛучшие параметры для Random Forest Classifier: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\nЛучший Accuracy (кросс-валидация): 0.8830\n\n=== Результаты на тестовой выборке ===\nTest Accuracy: 0.9389\nTest F1-Score: 0.9388\n\n=== Random Forest Regressor - Hyperparameter Tuning ===\nFitting 3 folds for each of 324 candidates, totalling 972 fits\nЛучшие параметры для Random Forest Regressor: {'max_depth': None, 'max_features': 0.8, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\nЛучший RMSE (кросс-валидация): 5.2533\n\n=== Результаты на тестовой выборке ===\nTest RMSE: 5.4137\nTest R²: 0.8863\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## 4. Имплементация алгоритма машинного обучения\n\nНапишем собственную реализацию случайного леса для классификации и регрессии, затем обучим модели на тестовых данных и сравним по качеству с реализациями из Sklearn. Будем использовать реализацию решающего дерева из ЛР №3.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tqdm import tqdm\n\nclass DecisionTreeNode:\n    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n        self.feature = feature\n        self.threshold = threshold\n        self.left = left\n        self.right = right\n        self.value = value\n\nclass DecisionTreeClassifierCustom:\n    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.root = None\n\n    def _gini(self, y):\n        if len(y) == 0:\n            return 0\n        proportions = np.bincount(y) / len(y)\n        return 1 - np.sum(proportions ** 2)\n\n    def _split(self, X, y, feature, threshold):\n        mask = X[:, feature] < threshold\n        return X[mask], y[mask], X[~mask], y[~mask]\n\n    def _best_split(self, X, y):\n        best_score = float('inf')\n        best_split = None\n\n        for feature in range(X.shape[1]):\n            thresholds = np.unique(X[:, feature])\n            for threshold in thresholds:\n                X_left, y_left, X_right, y_right = self._split(X, y, feature, threshold)\n\n                if len(y_left) < self.min_samples_leaf or len(y_right) < self.min_samples_leaf:\n                    continue\n\n                score = (len(y_left) * self._gini(y_left) + len(y_right) * self._gini(y_right)) / len(y)\n                if score < best_score:\n                    best_score = score\n                    best_split = (feature, threshold)\n\n        return best_split\n\n    def _build_tree(self, X, y, depth):\n        n_samples = len(y)\n        \n        if (depth == self.max_depth or n_samples < self.min_samples_split or \n            len(np.unique(y)) == 1):\n            return DecisionTreeNode(value=np.bincount(y).argmax() if n_samples > 0 else 0)\n\n        best_split = self._best_split(X, y)\n        if best_split is None:\n            return DecisionTreeNode(value=np.bincount(y).argmax())\n\n        feature, threshold = best_split\n        X_left, y_left, X_right, y_right = self._split(X, y, feature, threshold)\n\n        if len(y_left) < self.min_samples_leaf or len(y_right) < self.min_samples_leaf:\n            return DecisionTreeNode(value=np.bincount(y).argmax())\n\n        left_child = self._build_tree(X_left, y_left, depth + 1)\n        right_child = self._build_tree(X_right, y_right, depth + 1)\n\n        return DecisionTreeNode(feature, threshold, left_child, right_child)\n\n    def fit(self, X, y):\n        X = np.array(X)\n        y = np.array(y)\n        self.root = self._build_tree(X, y, 0)\n\n    def _predict(self, x, node):\n        if node.value is not None:\n            return node.value\n        if x[node.feature] < node.threshold:\n            return self._predict(x, node.left)\n        return self._predict(x, node.right)\n\n    def predict(self, X):\n        X = np.array(X)\n        return np.array([self._predict(x, self.root) for x in X])\n\nclass DecisionTreeRegressorCustom:\n    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.root = None\n\n    def _mse(self, y):\n        if len(y) == 0:\n            return 0\n        return np.mean((y - np.mean(y)) ** 2)\n\n    def _split(self, X, y, feature, threshold):\n        mask = X[:, feature] < threshold\n        return X[mask], y[mask], X[~mask], y[~mask]\n\n    def _best_split(self, X, y):\n        best_score = float('inf')\n        best_split = None\n\n        for feature in range(X.shape[1]):\n            thresholds = np.unique(X[:, feature])\n            for threshold in thresholds:\n                X_left, y_left, X_right, y_right = self._split(X, y, feature, threshold)\n\n                if len(y_left) < self.min_samples_leaf or len(y_right) < self.min_samples_leaf:\n                    continue\n\n                score = (len(y_left) * self._mse(y_left) + len(y_right) * self._mse(y_right)) / len(y)\n                if score < best_score:\n                    best_score = score\n                    best_split = (feature, threshold)\n\n        return best_split\n\n    def _build_tree(self, X, y, depth):\n        n_samples = len(y)\n        \n        if (depth == self.max_depth or n_samples < self.min_samples_split or \n            len(np.unique(y)) == 1):\n            return DecisionTreeNode(value=np.mean(y) if n_samples > 0 else 0)\n\n        best_split = self._best_split(X, y)\n        if best_split is None:\n            return DecisionTreeNode(value=np.mean(y))\n\n        feature, threshold = best_split\n        X_left, y_left, X_right, y_right = self._split(X, y, feature, threshold)\n\n        if len(y_left) < self.min_samples_leaf or len(y_right) < self.min_samples_leaf:\n            return DecisionTreeNode(value=np.mean(y))\n\n        left_child = self._build_tree(X_left, y_left, depth + 1)\n        right_child = self._build_tree(X_right, y_right, depth + 1)\n\n        return DecisionTreeNode(feature, threshold, left_child, right_child)\n\n    def fit(self, X, y):\n        X = np.array(X)\n        y = np.array(y)\n        self.root = self._build_tree(X, y, 0)\n\n    def _predict(self, x, node):\n        if node.value is not None:\n            return node.value\n        if x[node.feature] < node.threshold:\n            return self._predict(x, node.left)\n        return self._predict(x, node.right)\n\n    def predict(self, X):\n        X = np.array(X)\n        return np.array([self._predict(x, self.root) for x in X])\n\nclass CustomRandomForestClassifier:\n    def __init__(self, n_estimators=10, max_depth=None, min_samples_split=2, \n                 min_samples_leaf=1, max_features=None, random_state=42):\n        self.n_estimators = n_estimators\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.max_features = max_features\n        self.random_state = random_state\n        self.trees = []\n\n    def _bootstrap_sample(self, X, y):\n        n_samples = X.shape[0]\n        indices = np.random.choice(n_samples, n_samples, replace=True)\n        return X[indices], y[indices]\n\n    def fit(self, X, y):\n        np.random.seed(self.random_state)\n        self.trees = []\n        \n        for i in tqdm(range(self.n_estimators), desc=\"Training Random Forest Classifier\"):\n            X_sample, y_sample = self._bootstrap_sample(np.array(X), np.array(y))\n            tree = DecisionTreeClassifierCustom(\n                max_depth=self.max_depth,\n                min_samples_split=self.min_samples_split,\n                min_samples_leaf=self.min_samples_leaf\n            )\n            tree.fit(X_sample, y_sample)\n            self.trees.append(tree)\n\n    def predict(self, X):\n        X = np.array(X)\n        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n        return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=tree_predictions)\n\n\nclass CustomRandomForestRegressor:\n    def __init__(self, n_estimators=10, max_depth=None, min_samples_split=2, \n                 min_samples_leaf=1, max_features=None, random_state=42):\n        self.n_estimators = n_estimators\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.max_features = max_features\n        self.random_state = random_state\n        self.trees = []\n\n    def _bootstrap_sample(self, X, y):\n        n_samples = X.shape[0]\n        indices = np.random.choice(n_samples, n_samples, replace=True)\n        return X[indices], y[indices]\n\n    def fit(self, X, y):\n        np.random.seed(self.random_state)\n        self.trees = []\n        \n        for i in tqdm(range(self.n_estimators), desc=\"Training Random Forest Regressor\"):\n            X_sample, y_sample = self._bootstrap_sample(np.array(X), np.array(y))\n            tree = DecisionTreeRegressorCustom(\n                max_depth=self.max_depth,\n                min_samples_split=self.min_samples_split,\n                min_samples_leaf=self.min_samples_leaf\n            )\n            tree.fit(X_sample, y_sample)\n            self.trees.append(tree)\n\n    def predict(self, X):\n        X = np.array(X)\n        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n        return np.mean(tree_predictions, axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T20:28:55.571330Z","iopub.execute_input":"2025-12-14T20:28:55.571624Z","iopub.status.idle":"2025-12-14T20:28:55.611215Z","shell.execute_reply.started":"2025-12-14T20:28:55.571601Z","shell.execute_reply":"2025-12-14T20:28:55.610136Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"scaler = StandardScaler()\nX_train_class = scaler.fit_transform(X_train_class)\nX_test_class = scaler.transform(X_test_class)\n\ncustom_rf_classifier = CustomRandomForestClassifier(n_estimators=10, max_depth=5, random_state=42)\ncustom_rf_classifier.fit(X_train_class, y_train_class)\n\ny_pred_custom_class = custom_rf_classifier.predict(X_test_class)\n\naccuracy_custom = accuracy_score(y_test_class, y_pred_custom_class)\nf1_custom = f1_score(y_test_class, y_pred_custom_class, average='weighted')\n\nprint(f\"Custom Random Forest Classifier - Accuracy: {accuracy_custom:.4f}, F1-Score: {f1_custom:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T20:28:55.612277Z","iopub.execute_input":"2025-12-14T20:28:55.612601Z","iopub.status.idle":"2025-12-14T20:29:25.721023Z","shell.execute_reply.started":"2025-12-14T20:28:55.612569Z","shell.execute_reply":"2025-12-14T20:29:25.719730Z"}},"outputs":[{"name":"stderr","text":"Training Random Forest Classifier: 100%|██████████| 10/10 [00:30<00:00,  3.01s/it]","output_type":"stream"},{"name":"stdout","text":"Custom Random Forest Classifier - Accuracy: 0.8667, F1-Score: 0.8678\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"scaler = StandardScaler()\nX_train_reg = scaler.fit_transform(X_train_reg)\nX_test_reg = scaler.transform(X_test_reg)\n\ncustom_rf_regressor = CustomRandomForestRegressor(n_estimators=10, max_depth=5, random_state=42)\ncustom_rf_regressor.fit(X_train_reg, np.array(y_train_reg))\n\ny_pred_custom_reg = custom_rf_regressor.predict(X_test_reg)\n\nrmse_custom = mean_squared_error(y_test_reg, y_pred_custom_reg, squared = False)\nr2_custom = r2_score(y_test_reg, y_pred_custom_reg)\n\nprint(f\"Custom Random Forest Regressor - RMSE: {rmse_custom:.4f}, R²: {r2_custom:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T20:29:25.722185Z","iopub.execute_input":"2025-12-14T20:29:25.722476Z","iopub.status.idle":"2025-12-14T20:29:30.631554Z","shell.execute_reply.started":"2025-12-14T20:29:25.722456Z","shell.execute_reply":"2025-12-14T20:29:30.630509Z"}},"outputs":[{"name":"stderr","text":"Training Random Forest Regressor: 100%|██████████| 10/10 [00:04<00:00,  2.05it/s]","output_type":"stream"},{"name":"stdout","text":"Custom Random Forest Regressor - RMSE: 7.4933, R²: 0.7821\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"Точность вручную реализованных моделей оказалась похуже, чем у библиотечных, но все равно на приемлемом уровне.","metadata":{}}]}