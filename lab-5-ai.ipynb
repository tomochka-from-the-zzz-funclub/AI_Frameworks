{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3400552,"sourceType":"datasetVersion","datasetId":2049845},{"sourceId":7963387,"sourceType":"datasetVersion","datasetId":1328826}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Лабораторная работа №5: Проведение исследований с градиентным бустингом","metadata":{"id":"9VlZ9JhxWEFK"}},{"cell_type":"markdown","source":"## 1. Выбор начальных условий","metadata":{"id":"aeNbl03UWFrZ"}},{"cell_type":"markdown","source":"Был проведен в ЛР №1.","metadata":{"id":"n39vjl1fWHQm"}},{"cell_type":"code","source":"!pip install kagglehub scikit-learn numpy pandas matplotlib seaborn","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TSWkFqeOV-pP","outputId":"a9e02f25-08f4-42e0-ab0e-7cb4de46daa9","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T16:33:05.189218Z","iopub.execute_input":"2025-12-14T16:33:05.189880Z","iopub.status.idle":"2025-12-14T16:33:13.755318Z","shell.execute_reply.started":"2025-12-14T16:33:05.189854Z","shell.execute_reply":"2025-12-14T16:33:13.754025Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.13)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (25.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.10.5)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os","metadata":{"id":"-lSUi0y5WOfW","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T16:33:13.757294Z","iopub.execute_input":"2025-12-14T16:33:13.757632Z","iopub.status.idle":"2025-12-14T16:33:13.763598Z","shell.execute_reply.started":"2025-12-14T16:33:13.757602Z","shell.execute_reply":"2025-12-14T16:33:13.762594Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"date_fruit_path = \"/kaggle/input/date-fruit-datasets\"\n\nprint(\"Path to Date Fruit dataset files:\", date_fruit_path)\n\n# Проверка содержимого папки\nif os.path.exists(date_fruit_path):\n    files = os.listdir(date_fruit_path)\n    print(\"Files:\", files)\nelse:\n    print(\"Not found\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7rHGTZGFWSGc","outputId":"8b376486-f0fc-460d-a063-2996db17f6a0","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T16:33:13.764886Z","iopub.execute_input":"2025-12-14T16:33:13.765794Z","iopub.status.idle":"2025-12-14T16:33:13.807450Z","shell.execute_reply.started":"2025-12-14T16:33:13.765742Z","shell.execute_reply":"2025-12-14T16:33:13.806445Z"}},"outputs":[{"name":"stdout","text":"Path to Date Fruit dataset files: /kaggle/input/date-fruit-datasets\nFiles: ['Date_Fruit_Datasets']\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Чтение Excel-файла\ndate_fruit_data = pd.read_excel(f\"{date_fruit_path}/Date_Fruit_Datasets/Date_Fruit_Datasets.xlsx\")\n\n# Проверка данных\ndate_fruit_data.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GkR2xBMbWU6P","outputId":"5bd02dbd-9da0-4a6b-a30a-2def350dbcda","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T16:33:13.809397Z","iopub.execute_input":"2025-12-14T16:33:13.809895Z","iopub.status.idle":"2025-12-14T16:33:14.193996Z","shell.execute_reply.started":"2025-12-14T16:33:13.809871Z","shell.execute_reply":"2025-12-14T16:33:14.193104Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"     AREA  PERIMETER  MAJOR_AXIS  MINOR_AXIS  ECCENTRICITY   EQDIASQ  \\\n0  422163   2378.908    837.8484    645.6693        0.6373  733.1539   \n1  338136   2085.144    723.8198    595.2073        0.5690  656.1464   \n2  526843   2647.394    940.7379    715.3638        0.6494  819.0222   \n3  416063   2351.210    827.9804    645.2988        0.6266  727.8378   \n4  347562   2160.354    763.9877    582.8359        0.6465  665.2291   \n\n   SOLIDITY  CONVEX_AREA  EXTENT  ASPECT_RATIO  ...  KurtosisRR  KurtosisRG  \\\n0    0.9947       424428  0.7831        1.2976  ...      3.2370      2.9574   \n1    0.9974       339014  0.7795        1.2161  ...      2.6228      2.6350   \n2    0.9962       528876  0.7657        1.3150  ...      3.7516      3.8611   \n3    0.9948       418255  0.7759        1.2831  ...      5.0401      8.6136   \n4    0.9908       350797  0.7569        1.3108  ...      2.7016      2.9761   \n\n   KurtosisRB    EntropyRR    EntropyRG    EntropyRB  ALLdaub4RR  ALLdaub4RG  \\\n0      4.2287 -59191263232 -50714214400 -39922372608     58.7255     54.9554   \n1      3.1704 -34233065472 -37462601728 -31477794816     50.0259     52.8168   \n2      4.7192 -93948354560 -74738221056 -60311207936     65.4772     59.2860   \n3      8.2618 -32074307584 -32060925952 -29575010304     43.3900     44.1259   \n4      4.4146 -39980974080 -35980042240 -25593278464     52.7743     50.9080   \n\n   ALLdaub4RB  Class  \n0     47.8400  BERHI  \n1     47.8315  BERHI  \n2     51.9378  BERHI  \n3     41.1882  BERHI  \n4     42.6666  BERHI  \n\n[5 rows x 35 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AREA</th>\n      <th>PERIMETER</th>\n      <th>MAJOR_AXIS</th>\n      <th>MINOR_AXIS</th>\n      <th>ECCENTRICITY</th>\n      <th>EQDIASQ</th>\n      <th>SOLIDITY</th>\n      <th>CONVEX_AREA</th>\n      <th>EXTENT</th>\n      <th>ASPECT_RATIO</th>\n      <th>...</th>\n      <th>KurtosisRR</th>\n      <th>KurtosisRG</th>\n      <th>KurtosisRB</th>\n      <th>EntropyRR</th>\n      <th>EntropyRG</th>\n      <th>EntropyRB</th>\n      <th>ALLdaub4RR</th>\n      <th>ALLdaub4RG</th>\n      <th>ALLdaub4RB</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>422163</td>\n      <td>2378.908</td>\n      <td>837.8484</td>\n      <td>645.6693</td>\n      <td>0.6373</td>\n      <td>733.1539</td>\n      <td>0.9947</td>\n      <td>424428</td>\n      <td>0.7831</td>\n      <td>1.2976</td>\n      <td>...</td>\n      <td>3.2370</td>\n      <td>2.9574</td>\n      <td>4.2287</td>\n      <td>-59191263232</td>\n      <td>-50714214400</td>\n      <td>-39922372608</td>\n      <td>58.7255</td>\n      <td>54.9554</td>\n      <td>47.8400</td>\n      <td>BERHI</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>338136</td>\n      <td>2085.144</td>\n      <td>723.8198</td>\n      <td>595.2073</td>\n      <td>0.5690</td>\n      <td>656.1464</td>\n      <td>0.9974</td>\n      <td>339014</td>\n      <td>0.7795</td>\n      <td>1.2161</td>\n      <td>...</td>\n      <td>2.6228</td>\n      <td>2.6350</td>\n      <td>3.1704</td>\n      <td>-34233065472</td>\n      <td>-37462601728</td>\n      <td>-31477794816</td>\n      <td>50.0259</td>\n      <td>52.8168</td>\n      <td>47.8315</td>\n      <td>BERHI</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>526843</td>\n      <td>2647.394</td>\n      <td>940.7379</td>\n      <td>715.3638</td>\n      <td>0.6494</td>\n      <td>819.0222</td>\n      <td>0.9962</td>\n      <td>528876</td>\n      <td>0.7657</td>\n      <td>1.3150</td>\n      <td>...</td>\n      <td>3.7516</td>\n      <td>3.8611</td>\n      <td>4.7192</td>\n      <td>-93948354560</td>\n      <td>-74738221056</td>\n      <td>-60311207936</td>\n      <td>65.4772</td>\n      <td>59.2860</td>\n      <td>51.9378</td>\n      <td>BERHI</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>416063</td>\n      <td>2351.210</td>\n      <td>827.9804</td>\n      <td>645.2988</td>\n      <td>0.6266</td>\n      <td>727.8378</td>\n      <td>0.9948</td>\n      <td>418255</td>\n      <td>0.7759</td>\n      <td>1.2831</td>\n      <td>...</td>\n      <td>5.0401</td>\n      <td>8.6136</td>\n      <td>8.2618</td>\n      <td>-32074307584</td>\n      <td>-32060925952</td>\n      <td>-29575010304</td>\n      <td>43.3900</td>\n      <td>44.1259</td>\n      <td>41.1882</td>\n      <td>BERHI</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>347562</td>\n      <td>2160.354</td>\n      <td>763.9877</td>\n      <td>582.8359</td>\n      <td>0.6465</td>\n      <td>665.2291</td>\n      <td>0.9908</td>\n      <td>350797</td>\n      <td>0.7569</td>\n      <td>1.3108</td>\n      <td>...</td>\n      <td>2.7016</td>\n      <td>2.9761</td>\n      <td>4.4146</td>\n      <td>-39980974080</td>\n      <td>-35980042240</td>\n      <td>-25593278464</td>\n      <td>52.7743</td>\n      <td>50.9080</td>\n      <td>42.6666</td>\n      <td>BERHI</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 35 columns</p>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"concrete_strength_path = \"/kaggle/input/concrete-compressive-strength\"\n\nprint(\"Путь до датасета Concrete Compressive Strength:\", concrete_strength_path)\n\nif os.path.exists(concrete_strength_path):\n    files = os.listdir(concrete_strength_path)\n    print(\"Содержание:\", files)\nelse:\n    print(\"Не найден\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T16:33:14.195045Z","iopub.execute_input":"2025-12-14T16:33:14.195449Z","iopub.status.idle":"2025-12-14T16:33:14.203125Z","shell.execute_reply.started":"2025-12-14T16:33:14.195421Z","shell.execute_reply":"2025-12-14T16:33:14.202184Z"}},"outputs":[{"name":"stdout","text":"Путь до датасета Concrete Compressive Strength: /kaggle/input/concrete-compressive-strength\nСодержание: ['Concrete Compressive Strength.csv']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Чтение CSV-файла\nconcrete_data = pd.read_csv(f\"{concrete_strength_path}/Concrete Compressive Strength.csv\")\nconcrete_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T16:33:16.705064Z","iopub.execute_input":"2025-12-14T16:33:16.705380Z","iopub.status.idle":"2025-12-14T16:33:16.725578Z","shell.execute_reply.started":"2025-12-14T16:33:16.705358Z","shell.execute_reply":"2025-12-14T16:33:16.724386Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n0   540.0                 0.0      0.0  162.0               2.5   \n1   540.0                 0.0      0.0  162.0               2.5   \n2   332.5               142.5      0.0  228.0               0.0   \n3   332.5               142.5      0.0  228.0               0.0   \n4   198.6               132.4      0.0  192.0               0.0   \n\n   Coarse Aggregate  Fine Aggregate  Age (day)  Concrete compressive strength   \n0            1040.0           676.0         28                       79.986111  \n1            1055.0           676.0         28                       61.887366  \n2             932.0           594.0        270                       40.269535  \n3             932.0           594.0        365                       41.052780  \n4             978.4           825.5        360                       44.296075  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cement</th>\n      <th>Blast Furnace Slag</th>\n      <th>Fly Ash</th>\n      <th>Water</th>\n      <th>Superplasticizer</th>\n      <th>Coarse Aggregate</th>\n      <th>Fine Aggregate</th>\n      <th>Age (day)</th>\n      <th>Concrete compressive strength</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1040.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>79.986111</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>540.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>162.0</td>\n      <td>2.5</td>\n      <td>1055.0</td>\n      <td>676.0</td>\n      <td>28</td>\n      <td>61.887366</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>270</td>\n      <td>40.269535</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>332.5</td>\n      <td>142.5</td>\n      <td>0.0</td>\n      <td>228.0</td>\n      <td>0.0</td>\n      <td>932.0</td>\n      <td>594.0</td>\n      <td>365</td>\n      <td>41.052780</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>198.6</td>\n      <td>132.4</td>\n      <td>0.0</td>\n      <td>192.0</td>\n      <td>0.0</td>\n      <td>978.4</td>\n      <td>825.5</td>\n      <td>360</td>\n      <td>44.296075</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"## 2. Создание бейзлайна и оценка качества","metadata":{"id":"c4IjNZ6jWY3E"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score, make_scorer\nfrom sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.pipeline import Pipeline","metadata":{"id":"DVSuokOBWV26","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T16:33:21.474526Z","iopub.execute_input":"2025-12-14T16:33:21.475341Z","iopub.status.idle":"2025-12-14T16:33:22.061932Z","shell.execute_reply.started":"2025-12-14T16:33:21.475308Z","shell.execute_reply":"2025-12-14T16:33:22.060729Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"Разделим датасет для классификации на обучающую и тестовую выборки","metadata":{"id":"0dvDZGJOWfzg"}},{"cell_type":"code","source":"# Разделение на признаки и целевую переменную\nX_class = date_fruit_data.drop(columns=['Class'])\ny_class = date_fruit_data['Class']\n\n# Разделение на обучающую и тестовую выборки\nX_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n    X_class, y_class, test_size=0.2, random_state=42, stratify=y_class\n)\n\n# Преобразование целевой переменной\nlabel_encoder = LabelEncoder()\ny_train_class = label_encoder.fit_transform(y_train_class)\ny_test_class = label_encoder.transform(y_test_class)","metadata":{"id":"a-yxIrqSWgs5","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T16:33:24.802073Z","iopub.execute_input":"2025-12-14T16:33:24.802393Z","iopub.status.idle":"2025-12-14T16:33:24.830481Z","shell.execute_reply.started":"2025-12-14T16:33:24.802371Z","shell.execute_reply":"2025-12-14T16:33:24.829383Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"Аналогично разделим датасет для регрессии","metadata":{"id":"6bfeqKOjX0rR"}},{"cell_type":"code","source":"# Разделение на признаки и целевую переменную\nX_reg = concrete_data.drop(columns=['Concrete compressive strength '])\ny_reg = concrete_data['Concrete compressive strength ']\n\n# Разделение на обучающую и тестовую выборки\nX_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n    X_reg, y_reg, test_size=0.2, random_state=42\n)","metadata":{"id":"OCPVnm6EX1Pi","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T16:33:29.040576Z","iopub.execute_input":"2025-12-14T16:33:29.040970Z","iopub.status.idle":"2025-12-14T16:33:29.051202Z","shell.execute_reply.started":"2025-12-14T16:33:29.040945Z","shell.execute_reply":"2025-12-14T16:33:29.049800Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"Обучим модели для классификации и регрессии из Sklearn и оценим их","metadata":{"id":"x2JT9UbsX2n0"}},{"cell_type":"code","source":"gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\ngb_classifier.fit(X_train_class, y_train_class)\n\ny_pred_class = gb_classifier.predict(X_test_class)\n\naccuracy_gb = accuracy_score(y_test_class, y_pred_class)\nf1_gb = f1_score(y_test_class, y_pred_class, average='weighted')\n\nprint(f\"Gradient Boosting Classifier - Accuracy: {accuracy_gb:.4f}, F1-Score: {f1_gb:.4f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hEOMaxQyX4DH","outputId":"56943dad-b8c6-4ed9-f760-57580bc306d3","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T16:33:31.392532Z","iopub.execute_input":"2025-12-14T16:33:31.393320Z","iopub.status.idle":"2025-12-14T16:33:38.756999Z","shell.execute_reply.started":"2025-12-14T16:33:31.393296Z","shell.execute_reply":"2025-12-14T16:33:38.755948Z"}},"outputs":[{"name":"stdout","text":"Gradient Boosting Classifier - Accuracy: 0.8778, F1-Score: 0.8807\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\ngb_regressor.fit(X_train_reg, y_train_reg)\n\ny_pred_reg = gb_regressor.predict(X_test_reg)\n\nrmse_gb = mean_squared_error(y_test_reg, y_pred_reg, squared=False)\nr2_gb = r2_score(y_test_reg, y_pred_reg)\n\nprint(f\"Gradient Boosting Regressor - RMSE: {rmse_gb:.4f}, R²: {r2_gb:.4f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"juR7gdoAYB2P","outputId":"dd6e24fe-1ba5-42de-a5d1-bdeeb39e2134","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T16:34:25.833785Z","iopub.execute_input":"2025-12-14T16:34:25.834188Z","iopub.status.idle":"2025-12-14T16:34:26.038728Z","shell.execute_reply.started":"2025-12-14T16:34:25.834158Z","shell.execute_reply":"2025-12-14T16:34:26.037654Z"}},"outputs":[{"name":"stdout","text":"Gradient Boosting Regressor - RMSE: 5.5422, R²: 0.8808\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"Точность для встроенной в Sklearn модели градиентного бустинга для классификатора, получилась немного хуже, чем для случайного леса (87.8% точности против 92,2%), а для регрессора - на уровне случайного леса. Попробуем улучшить бейзлайн.","metadata":{"id":"-rMb1nQtYOIJ"}},{"cell_type":"markdown","source":"## 3. Улучшение бейзлайна","metadata":{"id":"bYAm-lSKYq0K"}},{"cell_type":"markdown","source":"Будем перебирать гиперпараметры градиентного бустинга, такие, как n_estimators, learning_rate, max_depth, min_samples_split, min_samples_leaf","metadata":{"id":"14ZYAJ3yYwhN"}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nimport numpy as np\n\n# === Подбор гиперпараметров для GradientBoostingClassifier ===\nprint(\"=== Gradient Boosting Classifier - Hyperparameter Tuning ===\")\nparam_dist_classifier = {\n    'n_estimators': [80, 100, 120],\n    'learning_rate': [0.05, 0.08, 0.1, 0.15],\n    'max_depth': [3, 4, 5],\n    'min_samples_split': [2, 3, 5],\n    'min_samples_leaf': [1, 2, 3],\n    'subsample': [0.8, 0.9, 1.0]\n}\n\ngb_classifier_search = RandomizedSearchCV(\n    estimator=GradientBoostingClassifier(random_state=42),\n    param_distributions=param_dist_classifier,\n    n_iter=15,              # Увеличили количество комбинаций\n    scoring='accuracy',\n    cv=3,\n    verbose=2,              # Более подробный вывод\n    random_state=42,\n    n_jobs=-1              # Использование всех ядер процессора\n)\n\ngb_classifier_search.fit(X_train_class, y_train_class)\n\n# Лучшие параметры и результат\nbest_gb_classifier_params = gb_classifier_search.best_params_\nbest_gb_classifier_score = gb_classifier_search.best_score_\n\nprint(f\"Лучшие параметры Gradient Boosting Classifier: {best_gb_classifier_params}\")\nprint(f\"Лучшая Accuracy (кросс-валидация): {best_gb_classifier_score:.4f}\")\n\n# === Подбор гиперпараметров для GradientBoostingRegressor ===\nprint(\"\\n=== Gradient Boosting Regressor - Hyperparameter Tuning ===\")\nparam_dist_regressor = {\n    'n_estimators': [80, 100, 120],\n    'learning_rate': [0.05, 0.08, 0.1, 0.15],\n    'max_depth': [3, 4, 5],\n    'min_samples_split': [2, 3, 5],\n    'min_samples_leaf': [1, 2, 3],\n    'subsample': [0.8, 0.9, 1.0],\n    'loss': ['squared_error', 'absolute_error']\n}\n\ngb_regressor_search = RandomizedSearchCV(\n    estimator=GradientBoostingRegressor(random_state=42),\n    param_distributions=param_dist_regressor,\n    n_iter=15,              # Увеличили количество комбинаций\n    scoring='neg_root_mean_squared_error',\n    cv=3,\n    verbose=2,\n    random_state=42,\n    n_jobs=-1\n)\n\ngb_regressor_search.fit(X_train_reg, y_train_reg)\n\n# Лучшие параметры и результат\nbest_gb_regressor_params = gb_regressor_search.best_params_\nbest_gb_regressor_score = -gb_regressor_search.best_score_\n\nprint(f\"Лучшие параметры Gradient Boosting Regressor: {best_gb_regressor_params}\")\nprint(f\"Лучший RMSE (кросс-валидация): {best_gb_regressor_score:.4f}\")\n\n# Дополнительная информация\nprint(f\"\\nВсего комбинаций для классификатора: {len(param_dist_classifier)}\")\nprint(f\"Всего комбинаций для регрессора: {len(param_dist_regressor)}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Am2aIScEZB1-","outputId":"4ba3d218-f910-4264-ac26-0074d54b0c9b","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T16:34:29.008103Z","iopub.execute_input":"2025-12-14T16:34:29.008407Z","iopub.status.idle":"2025-12-14T16:36:16.013891Z","shell.execute_reply.started":"2025-12-14T16:34:29.008387Z","shell.execute_reply":"2025-12-14T16:36:16.013005Z"}},"outputs":[{"name":"stdout","text":"=== Gradient Boosting Classifier - Hyperparameter Tuning ===\nFitting 3 folds for each of 15 candidates, totalling 45 fits\nЛучшие параметры Gradient Boosting Classifier: {'subsample': 0.8, 'n_estimators': 80, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': 4, 'learning_rate': 0.15}\nЛучшая Accuracy (кросс-валидация): 0.8704\n\n=== Gradient Boosting Regressor - Hyperparameter Tuning ===\nFitting 3 folds for each of 15 candidates, totalling 45 fits\nЛучшие параметры Gradient Boosting Regressor: {'subsample': 0.8, 'n_estimators': 80, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 5, 'loss': 'squared_error', 'learning_rate': 0.15}\nЛучший RMSE (кросс-валидация): 4.6814\n\nВсего комбинаций для классификатора: 6\nВсего комбинаций для регрессора: 7\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"Обучим модели с лучшими гиперпараметрами","metadata":{"id":"z51vJIEJcrYK"}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score\nimport numpy as np\n\n# GradientBoostingClassifier с подобранными параметрами\nprint(\"=== Оценка лучшего Gradient Boosting Classifier ===\")\noptimized_gb_classifier = GradientBoostingClassifier(\n    n_estimators=best_gb_classifier_params['n_estimators'],\n    learning_rate=best_gb_classifier_params['learning_rate'],\n    max_depth=best_gb_classifier_params['max_depth'],\n    min_samples_split=best_gb_classifier_params['min_samples_split'],\n    min_samples_leaf=best_gb_classifier_params['min_samples_leaf'],\n    subsample=best_gb_classifier_params.get('subsample', 1.0),\n    random_state=42\n)\n\n# Обучение модели\noptimized_gb_classifier.fit(X_train_class, y_train_class)\n\n# Предсказания\ny_pred_class = optimized_gb_classifier.predict(X_test_class)\ny_pred_prob_class = optimized_gb_classifier.predict_proba(X_test_class)[:, 1]\n\n# Метрики\naccuracy = accuracy_score(y_test_class, y_pred_class)\nf1 = f1_score(y_test_class, y_pred_class, average='weighted')\n\nprint(f\"Оптимальные параметры модели: {best_gb_classifier_params}\")\nprint(f\"Accuracy на тестовой выборке: {accuracy:.4f}\")\nprint(f\"F1-Score (weighted): {f1:.4f}\")\nprint(f\"Количество деревьев: {best_gb_classifier_params['n_estimators']}\")\nprint(f\"Скорость обучения: {best_gb_classifier_params['learning_rate']}\")\n\n# GradientBoostingRegressor с подобранными параметрами\nprint(\"\\n=== Оценка лучшего Gradient Boosting Regressor ===\")\noptimized_gb_regressor = GradientBoostingRegressor(\n    n_estimators=best_gb_regressor_params['n_estimators'],\n    learning_rate=best_gb_regressor_params['learning_rate'],\n    max_depth=best_gb_regressor_params['max_depth'],\n    min_samples_split=best_gb_regressor_params['min_samples_split'],\n    min_samples_leaf=best_gb_regressor_params['min_samples_leaf'],\n    subsample=best_gb_regressor_params.get('subsample', 1.0),\n    loss=best_gb_regressor_params.get('loss', 'squared_error'),  # Безопасное получение параметра\n    random_state=42\n)\n\n# Обучение модели\noptimized_gb_regressor.fit(X_train_reg, y_train_reg)\n\n# Предсказания\ny_pred_reg = optimized_gb_regressor.predict(X_test_reg)\n\n# Метрики\nmse = mean_squared_error(y_test_reg, y_pred_reg)\nrmse = np.sqrt(mse)\nr2 = r2_score(y_test_reg, y_pred_reg)\nmae = np.mean(np.abs(y_test_reg - y_pred_reg))\n\nprint(f\"Оптимальные параметры модели: {best_gb_regressor_params}\")\nprint(f\"RMSE на тестовой выборке: {rmse:.4f}\")\nprint(f\"R² Score: {r2:.4f}\")\nprint(f\"MAE: {mae:.4f}\")\nprint(f\"MSE: {mse:.4f}\")\nprint(f\"Количество деревьев: {best_gb_regressor_params['n_estimators']}\")\nprint(f\"Скорость обучения: {best_gb_regressor_params['learning_rate']}\")\nprint(\"\\n=== Сводная информация ===\")\nprint(f\"Классификатор - Accuracy improvement: {(accuracy - best_gb_classifier_score):.4f}\")\nprint(f\"Регрессор - RMSE improvement: {(best_gb_regressor_score - rmse):.4f}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_KUYwoVvcpoQ","outputId":"82b79bfc-73e5-4e3d-924c-954d878b0d35","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T17:16:27.202273Z","iopub.execute_input":"2025-12-14T17:16:27.202606Z","iopub.status.idle":"2025-12-14T17:16:33.454946Z","shell.execute_reply.started":"2025-12-14T17:16:27.202576Z","shell.execute_reply":"2025-12-14T17:16:33.454113Z"}},"outputs":[{"name":"stdout","text":"=== Оценка лучшего Gradient Boosting Classifier ===\nОптимальные параметры модели: {'subsample': 0.8, 'n_estimators': 80, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': 4, 'learning_rate': 0.15}\nAccuracy на тестовой выборке: 0.8944\nF1-Score (weighted): 0.8962\nКоличество деревьев: 80\nСкорость обучения: 0.15\n\n=== Оценка лучшего Gradient Boosting Regressor ===\nОптимальные параметры модели: {'subsample': 0.8, 'n_estimators': 80, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 5, 'loss': 'squared_error', 'learning_rate': 0.15}\nRMSE на тестовой выборке: 4.4988\nR² Score: 0.9215\nMAE: 3.0811\nMSE: 20.2389\nКоличество деревьев: 80\nСкорость обучения: 0.15\n\n=== Сводная информация ===\nКлассификатор - Accuracy improvement: 0.0240\nРегрессор - RMSE improvement: 0.1826\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"Точность для классификации выросла ненамного, а вот для регрессии, кажется, это лучший результат среди всех рассмотренных моделей в работах.","metadata":{"id":"637JuO-Mc89y"}},{"cell_type":"markdown","source":"## 4. Имплементация алгоритма машинного обучения","metadata":{"id":"NScYDvqBdOLZ"}},{"cell_type":"markdown","source":"Напишем собственную реализацию градиентного бустинга для классификации и регрессии, затем обучим модели на тестовых данных и сравним по качеству с реализациями из Sklearn. Будем использовать реализацию решающего дерева из ЛР №3.","metadata":{"id":"7j1LoCl1dQFj"}},{"cell_type":"code","source":"import numpy as np\nfrom tqdm import tqdm\n\nclass DecisionTreeNode:\n    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n        self.feature = feature\n        self.threshold = threshold\n        self.left = left\n        self.right = right\n        self.value = value\n\nclass DecisionTreeRegressorCustom:\n    def __init__(self, max_depth=3, min_samples_split=2, min_samples_leaf=1):\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.min_samples_leaf = min_samples_leaf\n        self.root = None\n\n    def _mse(self, y):\n        if len(y) == 0:\n            return 0\n        return np.mean((y - np.mean(y)) ** 2)\n\n    def _split(self, X, y, feature, threshold):\n        mask = X[:, feature] < threshold\n        return X[mask], y[mask], X[~mask], y[~mask]\n\n    def _best_split(self, X, y):\n        best_score = float('inf')\n        best_split = None\n\n        n_features = X.shape[1]\n        for feature in range(n_features):\n            # Берем только несколько случайных порогов для скорости\n            unique_vals = np.unique(X[:, feature])\n            if len(unique_vals) > 10:  # Ограничиваем количество порогов\n                thresholds = np.random.choice(unique_vals, size=10, replace=False)\n            else:\n                thresholds = unique_vals\n            \n            for threshold in thresholds:\n                X_left, y_left, X_right, y_right = self._split(X, y, feature, threshold)\n\n                if len(y_left) < self.min_samples_leaf or len(y_right) < self.min_samples_leaf:\n                    continue\n\n                score = (len(y_left) * self._mse(y_left) + len(y_right) * self._mse(y_right)) / len(y)\n                if score < best_score:\n                    best_score = score\n                    best_split = (feature, threshold)\n\n        return best_split\n\n    def _build_tree(self, X, y, depth):\n        n_samples = len(y)\n        \n        if (depth >= self.max_depth or \n            n_samples < self.min_samples_split or \n            len(np.unique(y)) == 1 or\n            n_samples == 0):\n            return DecisionTreeNode(value=np.mean(y) if n_samples > 0 else 0)\n\n        # Находим лучшее разбиение\n        best_split = self._best_split(X, y)\n        if best_split is None:\n            return DecisionTreeNode(value=np.mean(y))\n\n        feature, threshold = best_split\n        \n        # Разделяем данные\n        X_left, y_left, X_right, y_right = self._split(X, y, feature, threshold)\n        \n        # Если после разделения одна из частей слишком мала\n        if len(y_left) < self.min_samples_leaf or len(y_right) < self.min_samples_leaf:\n            return DecisionTreeNode(value=np.mean(y))\n\n        # Рекурсивно строим поддеревья\n        left_child = self._build_tree(X_left, y_left, depth + 1)\n        right_child = self._build_tree(X_right, y_right, depth + 1)\n\n        return DecisionTreeNode(\n            feature=feature,\n            threshold=threshold,\n            left=left_child,\n            right=right_child\n        )\n\n    def fit(self, X, y):\n        X = np.array(X, dtype=np.float32)\n        y = np.array(y, dtype=np.float32)\n        self.root = self._build_tree(X, y, 0)\n\n    def _predict(self, x, node):\n        if node.value is not None:\n            return node.value\n        if x[node.feature] < node.threshold:\n            return self._predict(x, node.left)\n        return self._predict(x, node.right)\n\n    def predict(self, X):\n        X = np.array(X, dtype=np.float32)\n        return np.array([self._predict(x, self.root) for x in X])\n\nclass CustomGradientBoostingClassifier:\n    def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=3, verbose=True):\n        self.n_estimators = n_estimators\n        self.learning_rate = learning_rate\n        self.max_depth = max_depth\n        self.verbose = verbose\n        self.models = []\n        self.init_pred = None\n        self.n_classes = None\n\n    def _softmax(self, log_odds):\n        exp_log_odds = np.exp(log_odds - np.max(log_odds, axis=1, keepdims=True))\n        return exp_log_odds / np.sum(exp_log_odds, axis=1, keepdims=True)\n\n    def fit(self, X, y):\n        X = np.array(X, dtype=np.float32)\n        y = np.array(y, dtype=np.int32)\n        \n        self.n_classes = len(np.unique(y))\n        if self.n_classes < 2:\n            raise ValueError(\"Должно быть как минимум 2 класса\")\n        \n        # Инициализация предсказаний\n        class_counts = np.bincount(y, minlength=self.n_classes)\n        self.init_pred = np.log((class_counts + 1e-8) / len(y))\n        \n        # Начальные предсказания\n        current_pred = np.tile(self.init_pred, (len(y), 1))\n        self.models = []\n\n        iterator = range(self.n_estimators)\n        if self.verbose:\n            iterator = tqdm(iterator, desc=\"GB Classifier Training\")\n        \n        for i in iterator:\n            probs = self._softmax(current_pred)\n            \n            # Вычисляем градиенты\n            residuals = np.zeros_like(current_pred)\n            for c in range(self.n_classes):\n                y_mask = (y == c).astype(np.float32)\n                residuals[:, c] = probs[:, c] - y_mask\n            \n            # Обучаем деревья для каждого класса\n            trees = []\n            for c in range(self.n_classes):\n                tree = DecisionTreeRegressorCustom(\n                    max_depth=self.max_depth,\n                    min_samples_split=2,\n                    min_samples_leaf=1\n                )\n                tree.fit(X, residuals[:, c])\n                trees.append(tree)\n            \n            self.models.append(trees)\n            \n            # Обновляем предсказания\n            for c, tree in enumerate(trees):\n                current_pred[:, c] += self.learning_rate * tree.predict(X)\n            \n            if self.verbose and i % 5 == 0:\n                current_acc = np.mean(np.argmax(probs, axis=1) == y)\n                iterator.set_postfix({'acc': f'{current_acc:.3f}'})\n\n    def predict_proba(self, X):\n        X = np.array(X, dtype=np.float32)\n        pred = np.tile(self.init_pred, (X.shape[0], 1))\n        \n        for trees in self.models:\n            for c, tree in enumerate(trees):\n                pred[:, c] += self.learning_rate * tree.predict(X)\n        \n        return self._softmax(pred)\n\n    def predict(self, X):\n        probs = self.predict_proba(X)\n        return np.argmax(probs, axis=1)","metadata":{"id":"WWlpXgTPdiHX","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T16:59:08.202187Z","iopub.execute_input":"2025-12-14T16:59:08.202519Z","iopub.status.idle":"2025-12-14T16:59:08.229660Z","shell.execute_reply.started":"2025-12-14T16:59:08.202497Z","shell.execute_reply":"2025-12-14T16:59:08.228607Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"Обучим модели и оценим их качество","metadata":{"id":"Fz84yTlxdsQz"}},{"cell_type":"code","source":"# Обучаем кастомный классификатор градиентного бустинга\ncustom_gb_classifier = CustomGradientBoostingClassifier(\n    n_estimators=10, \n    learning_rate=0.1, \n    max_depth=3,\n    verbose=True\n)\n\n# Обучение модели\ncustom_gb_classifier.fit(X_train_class, y_train_class)\n\n# Предсказания\ny_pred_custom_class = custom_gb_classifier.predict(X_test_class)\n\n# Оценка качества\naccuracy_custom = accuracy_score(y_test_class, y_pred_custom_class)\nf1_custom = f1_score(y_test_class, y_pred_custom_class, average='weighted')\n\n# Вывод результатов\nprint(f\"\\n=== Кастомный Gradient Boosting Classifier ===\")\nprint(f\"Accuracy: {accuracy_custom:.4f}\")\nprint(f\"F1-Score: {f1_custom:.4f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yOCH_BeSdu3D","outputId":"af9db6f1-4364-4451-8221-c32e24b87b0e","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T16:59:11.390842Z","iopub.execute_input":"2025-12-14T16:59:11.391644Z","iopub.status.idle":"2025-12-14T16:59:26.379987Z","shell.execute_reply.started":"2025-12-14T16:59:11.391618Z","shell.execute_reply":"2025-12-14T16:59:26.378883Z"}},"outputs":[{"name":"stderr","text":"GB Classifier Training: 100%|██████████| 10/10 [00:14<00:00,  1.50s/it, acc=0.000]","output_type":"stream"},{"name":"stdout","text":"\n=== Кастомный Gradient Boosting Classifier ===\nAccuracy: 0.0056\nF1-Score: 0.0026\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"scaler = StandardScaler()\nX_train_reg = scaler.fit_transform(X_train_reg)\nX_test_reg = scaler.transform(X_test_reg)\n\n# Создаем и обучаем кастомный регрессор\ncustom_gb_regressor = CustomGradientBoostingRegressor(\n    n_estimators=10, \n    learning_rate=0.1, \n    max_depth=3,\n    verbose=True\n)\n\ncustom_gb_regressor.fit(X_train_reg, y_train_reg)\n\n# Предсказания\ny_pred_custom_reg = custom_gb_regressor.predict(X_test_reg)\n\n# Метрики качества\nrmse_custom = mean_squared_error(y_test_reg, y_pred_custom_reg, squared=False)\nr2_custom = r2_score(y_test_reg, y_pred_custom_reg)\n\n# Вывод результатов\nprint(f\"\\n=== Кастомный Gradient Boosting Regressor ===\")\nprint(f\"RMSE: {rmse_custom:.4f}\")\nprint(f\"R² Score: {r2_custom:.4f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hqo6opDAibRu","outputId":"a1e28d87-93bd-4d8b-b5d1-26dc938f194f","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T17:06:55.839121Z","iopub.execute_input":"2025-12-14T17:06:55.839453Z","iopub.status.idle":"2025-12-14T17:06:56.363854Z","shell.execute_reply.started":"2025-12-14T17:06:55.839430Z","shell.execute_reply":"2025-12-14T17:06:56.362862Z"}},"outputs":[{"name":"stderr","text":"GB Regressor Training: 100%|██████████| 10/10 [00:00<00:00, 19.74it/s]","output_type":"stream"},{"name":"stdout","text":"\n=== Кастомный Gradient Boosting Regressor ===\nRMSE: 10.2992\nR² Score: 0.5884\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"Модель классификации сравнительно долго работала, в отличии от регрессии, но и показала плохие результаты. Модель регрессии показала средние, но приемлимые результаты.","metadata":{"id":"wMMpZ84anRpo"}}]}
