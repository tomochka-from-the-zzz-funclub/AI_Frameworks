{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3400552,"sourceType":"datasetVersion","datasetId":2049845},{"sourceId":7963387,"sourceType":"datasetVersion","datasetId":1328826}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Лабораторная работа №2: Проведение исследований с логистической и линейной регрессией","metadata":{}},{"cell_type":"markdown","source":"### 1. Выбор начальных условий","metadata":{}},{"cell_type":"code","source":"!pip install scikit-learn numpy pandas matplotlib seaborn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-07T17:32:03.047265Z","iopub.execute_input":"2025-12-07T17:32:03.047610Z","iopub.status.idle":"2025-12-07T17:32:07.225436Z","shell.execute_reply.started":"2025-12-07T17:32:03.047583Z","shell.execute_reply":"2025-12-07T17:32:07.224196Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T17:32:07.227178Z","iopub.execute_input":"2025-12-07T17:32:07.227550Z","iopub.status.idle":"2025-12-07T17:32:07.232784Z","shell.execute_reply.started":"2025-12-07T17:32:07.227519Z","shell.execute_reply":"2025-12-07T17:32:07.231883Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"date_fruit_path = \"/kaggle/input/date-fruit-datasets\"\n\nprint(\"Path to Date Fruit dataset files:\", date_fruit_path)\n\n# Проверка содержимого папки\nif os.path.exists(date_fruit_path):\n    files = os.listdir(date_fruit_path)\n    print(\"Files:\", files)\nelse:\n    print(\"Not found\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T17:32:07.233575Z","iopub.execute_input":"2025-12-07T17:32:07.233898Z","iopub.status.idle":"2025-12-07T17:32:07.269001Z","shell.execute_reply.started":"2025-12-07T17:32:07.233871Z","shell.execute_reply":"2025-12-07T17:32:07.267887Z"}},"outputs":[{"name":"stdout","text":"Path to Date Fruit dataset files: /kaggle/input/date-fruit-datasets\nFiles: ['Date_Fruit_Datasets']\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Чтение Excel-файла\ndate_fruit_data = pd.read_excel(f\"{date_fruit_path}/Date_Fruit_Datasets/Date_Fruit_Datasets.xlsx\")\n\n# Проверка данных\ndate_fruit_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T17:32:07.271349Z","iopub.execute_input":"2025-12-07T17:32:07.271881Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"concrete_strength_path = \"/kaggle/input/concrete-compressive-strength\"\n\nprint(\"Путь до датасета Concrete Compressive Strength:\", concrete_strength_path)\n\n# Проверка содержимого папки\nif os.path.exists(concrete_strength_path):\n    files = os.listdir(concrete_strength_path)\n    print(\"Содержание:\", files)\nelse:\n    print(\"Не найден\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Чтение CSV-файла\nconcrete_data = pd.read_csv(f\"{concrete_strength_path}/Concrete Compressive Strength.csv\")\n\n# Проверка данных\nconcrete_data.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. Создание бейзлайна и оценка качества","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score, make_scorer\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.pipeline import Pipeline","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Разделим датасет для классификации на обучающую и тестовую выборки","metadata":{}},{"cell_type":"code","source":"# Разделение на признаки и целевую переменную\nX_class = date_fruit_data.drop(columns=['Class'])\ny_class = date_fruit_data['Class']\n\n# Разделение на обучающую и тестовую выборки\nX_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n    X_class, y_class, test_size=0.2, random_state=42, stratify=y_class\n)\n\n# Преобразование целевой переменной\nlabel_encoder = LabelEncoder()\ny_train_class = label_encoder.fit_transform(y_train_class)\ny_test_class = label_encoder.transform(y_test_class)\n     ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Аналогично разделим датасет для регрессии","metadata":{}},{"cell_type":"code","source":"# Разделение на признаки и целевую переменную\nX_reg = concrete_data.drop(columns=['Concrete compressive strength '])\ny_reg = concrete_data['Concrete compressive strength ']\n\n# Разделение на обучающую и тестовую выборки\nX_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n    X_reg, y_reg, test_size=0.2, random_state=42\n)\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Обучим модели для классификации и регрессии из Sklearn и оценим их. Для логистической регрессии нам требуется обязательный препроцессинг данных с помощью StandardScaler.","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nX_train_class = scaler.fit_transform(X_train_class)\nX_test_class = scaler.transform(X_test_class)\n\nlogistic = LogisticRegression(max_iter=10000)\nlogistic.fit(X_train_class, y_train_class)\n\ny_pred_class = logistic.predict(X_test_class)\n\naccuracy = accuracy_score(y_test_class, y_pred_class)\nf1 = f1_score(y_test_class, y_pred_class, average='weighted')\n\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"F1-Score: {f1:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"linear_regressor = LinearRegression()\nlinear_regressor.fit(X_train_reg, y_train_reg)\n\ny_pred_reg = linear_regressor.predict(X_test_reg)\n\nrmse = mean_squared_error(y_test_reg, y_pred_reg, squared=False)\nr2 = r2_score(y_test_reg, y_pred_reg)\n\nprint(f\"RMSE: {rmse:.4f}\")\nprint(f\"R²: {r2:.4f}\")\n     ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Итак, точность для встроенной в Sklearn модели логистической регрессии получилась отличной (92.7% точности). Для модели линейной регрессии среднеквадратичная ошибка составила 9.79, что несколько хуже бейзлайна для KNN-регрессора. Попробуем улучшить бейзлайн.","metadata":{}},{"cell_type":"markdown","source":"### 3. Улучшение бейзлайна\n\nДля улучшения бейзлайна для логистичекой регрессии будем подбирать гиперпараметр C с помощью GridSearchCV. Для линейной регрессии добавим регуляризацию через Ridge (L2).","metadata":{}},{"cell_type":"code","source":"\n\n# Пайплайн с нормализацией и логистической регрессией\npipeline_logistic = Pipeline(steps=[\n    ('scaler', StandardScaler()),\n    ('logistic', LogisticRegression(max_iter=10000, random_state=42))\n])\n\n# Параметры для подбора\nparam_grid_logistic = {\n    'logistic__C': [0.01, 0.1, 1, 10, 75, 1000],\n    'logistic__penalty': ['l2'],\n    'logistic__solver': ['lbfgs']\n}\n\n# Подбор гиперпараметров\ngrid_search_logistic = GridSearchCV(\n    pipeline_logistic,\n    param_grid_logistic,\n    cv=5,\n    scoring='accuracy',\n    verbose=1,\n    n_jobs=-1\n)\n\ngrid_search_logistic.fit(X_train_class, y_train_class)\n\nbest_params_logistic = grid_search_logistic.best_params_\nbest_score_logistic = grid_search_logistic.best_score_\n\nprint(\"\\n\" + \"=\"*50)\nprint(f\"Лучшие параметры для Logistic Regression: {best_params_logistic}\")\nprint(f\"Лучший Accuracy на кросс-валидации: {best_score_logistic:.4f}\")\n\n# Оценка на тестовой выборке\ny_pred_class = grid_search_logistic.best_estimator_.predict(X_test_class)\naccuracy = accuracy_score(y_test_class, y_pred_class)\nf1 = f1_score(y_test_class, y_pred_class, average='weighted')\n\nprint(f\"Test Accuracy: {accuracy:.4f}\")\nprint(f\"Test F1-Score: {f1:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"param_grid_ridge = {\n    'alpha': [0.001, 0.01, 0.1, 1, 5, 10, 50, 100, 500, 1000, 5000]  \n}\n\ngrid_search_ridge = GridSearchCV(\n    Ridge(random_state=42), \n    param_grid_ridge,\n    cv=5,\n    scoring=['neg_root_mean_squared_error', 'r2'],  \n    refit='neg_root_mean_squared_error',  \n    verbose=1,\n    n_jobs=-1 \n)\n\ngrid_search_ridge.fit(X_train_reg, y_train_reg)\n\nbest_params_ridge = grid_search_ridge.best_params_\nbest_rmse_score = -grid_search_ridge.best_score_ \n\ncv_results = grid_search_ridge.cv_results_\n\nprint(f\"Лучшие параметры для Ridge Regression: {best_params_ridge}\")\nprint(f\"Лучший RMSE на кросс-валидации: {best_rmse_score:.4f}\")\n\nbest_idx = grid_search_ridge.best_index_\nprint(f\"Лучшее alpha: {best_params_ridge['alpha']}\")\nprint(f\"Соответствующий R² на кросс-валидации: {cv_results['mean_test_r2'][best_idx]:.4f}\")\n\n# Оценка на тестовой выборке\nridge_regressor = grid_search_ridge.best_estimator_\ny_pred_reg = ridge_regressor.predict(X_test_reg)\n\nrmse = mean_squared_error(y_test_reg, y_pred_reg, squared=False)\nr2 = r2_score(y_test_reg, y_pred_reg)\n\nprint(f\"\\nРезультаты на тестовой выборке:\")\nprint(f\"Test RMSE: {rmse:.4f}\")\nprint(f\"Test R²:   {r2:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Итак, результаты лишь немного улучшились по сравнению с бейзлайном.","metadata":{}},{"cell_type":"markdown","source":"### 4. Имплементация алгоритма машинного обучения\nНапишем собственную реализацию Logistic and Linear regression, затем обучим модели на тестовых данных и сравним по качеству с реализациями из Sklearn.","metadata":{}},{"cell_type":"code","source":"sigmoid_function = lambda z: np.clip(1 / (1 + np.exp(-np.clip(z, -500, 500))), 1e-10, 1 - 1e-10)\n\ndef optimize_gradients(gradient_fn, start_point, learning_rate, max_iter, tolerance=1e-6):\n    current_point = start_point\n    for i in range(max_iter):\n        grad = gradient_fn(current_point)\n        current_point -= learning_rate * grad\n        \n        if np.linalg.norm(grad) < tolerance:\n            print(f\"Градиент сошелся на итерации {i}\")\n            break\n    return current_point\n\n\nclass CustomLogisticRegression:\n    def __init__(self, *, lr=0.01, max_epochs=1000, add_intercept=True):\n        self._learning_rate = lr\n        self._max_epochs = max_epochs\n        self._add_intercept = add_intercept\n        self._parameters = None\n        self._X_train = None\n        self._y_train = None\n        self._X_train_mean = None\n        self._X_train_std = None\n\n    def _add_intercept_to_X(self, X):\n        if self._add_intercept:\n            return np.hstack([np.ones((X.shape[0], 1)), X])\n        return X\n\n    def _compute_gradient(self, params):\n        assert self._X_train is not None\n        assert self._y_train is not None\n\n        samples = self._X_train.shape[0]\n        predictions = sigmoid_function(np.dot(self._X_train, params))\n        \n        grad = np.dot(self._X_train.T, (predictions - self._y_train)) / samples\n        return grad\n\n    def fit(self, X, y):\n        assert self._parameters is None\n\n        X_array = np.array(X)\n        y_array = np.array(y).reshape(-1, 1)\n        \n        # Нормализация признаков для стабильности\n        if X_array.shape[1] > 0:\n            self._X_train_mean = np.mean(X_array, axis=0)\n            self._X_train_std = np.std(X_array, axis=0) + 1e-8\n            X_normalized = (X_array - self._X_train_mean) / self._X_train_std\n            self._X_train = self._add_intercept_to_X(X_normalized)\n        else:\n            self._X_train = self._add_intercept_to_X(X_array)\n        \n        self._y_train = y_array\n        features = self._X_train.shape[1]\n        \n        initial_params = np.zeros((features, 1))\n        \n        self._parameters = optimize_gradients(\n            self._compute_gradient, \n            initial_params, \n            self._learning_rate, \n            self._max_epochs\n        ).flatten()\n\n    def predict_proba(self, X):\n        assert self._parameters is not None\n        \n        X_array = np.array(X)\n        \n        # Применяем ту же нормализацию, что при обучении\n        if self._X_train_mean is not None:\n            X_normalized = (X_array - self._X_train_mean) / self._X_train_std\n            X_with_intercept = self._add_intercept_to_X(X_normalized)\n        else:\n            X_with_intercept = self._add_intercept_to_X(X_array)\n            \n        z = np.dot(X_with_intercept, self._parameters)\n        return sigmoid_function(z)\n\n    def predict(self, X, threshold=0.5):\n        probabilities = self.predict_proba(X)\n        return (probabilities >= threshold).astype(int)\n\n\nclass CustomLinearRegression:\n    def __init__(self, add_intercept=True):\n        self._add_intercept = add_intercept\n        self._coef_ = None\n        self._X_mean = None\n        self._X_std = None\n\n    def _add_intercept_to_X(self, X):\n        if self._add_intercept:\n            return np.hstack([np.ones((X.shape[0], 1)), X])\n        return X\n\n    def fit(self, X, y):\n        X_array = np.array(X)\n        y_array = np.array(y).reshape(-1, 1)\n        \n        # Нормализация признаков\n        if X_array.shape[1] > 0:\n            self._X_mean = np.mean(X_array, axis=0)\n            self._X_std = np.std(X_array, axis=0) + 1e-8\n            X_normalized = (X_array - self._X_mean) / self._X_std\n            X_intercept = self._add_intercept_to_X(X_normalized)\n        else:\n            X_intercept = self._add_intercept_to_X(X_array)\n        \n        self._coef_ = np.linalg.pinv(X_intercept) @ y_array\n        \n        return self\n\n    def predict(self, X):\n        X_array = np.array(X)\n        \n        # Применяем нормализацию\n        if self._X_mean is not None:\n            X_normalized = (X_array - self._X_mean) / self._X_std\n            X_intercept = self._add_intercept_to_X(X_normalized)\n        else:\n            X_intercept = self._add_intercept_to_X(X_array)\n            \n        return (X_intercept @ self._coef_).flatten()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Обучим модели и оценим их качество","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score\n\nlog_reg = CustomLogisticRegression(lr=0.1, max_epochs=1000)\nlog_reg.fit(X_train_class, y_train_class)\ny_pred_class = log_reg.predict(X_test_class)\n\naccuracy = accuracy_score(y_test_class, y_pred_class)\nf1 = f1_score(y_test_class, y_pred_class, average=\"weighted\")\n\n\nprint(f\"Custom Logistic Regression - Accuracy: {accuracy:.4f}, F1-Score: {f1:.4f}\")\n\nlin_reg = CustomLinearRegression()\nlin_reg.fit(X_train_reg, y_train_reg)\ny_pred_reg = lin_reg.predict(X_test_reg)\n\nrmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))\nr2 = r2_score(y_test_reg, y_pred_reg)\n\nprint(f\"Custom Linear Regression - RMSE: {rmse:.4f}, R²: {r2:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Точность и F1-Score логистической модели сильно упали, а показатели линейной - остались примерно теми же.","metadata":{}}]}